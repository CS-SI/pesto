{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"PESTO : ProcESsing facTOry PESTO is a packaging tool inspired from pip, maven and similar dependencies managers. It contains shell tools to generate all the boiler plate to build an OpenAPI processing web service compliant with the Geoprocessing-API . PESTO is designed to ease the process of packaging a Python algorithm as a processing web service into a docker image. The deployment of a web service becomes now as easy as filling few configuration files. PESTO is composed of the following components: PESTO-CLI : the command line interface used to create a pesto project and package processing algorithms. PESTO-PROJ : the project workspace created by PESTO to configure the packaging process of your algorithm. PESTO-PWS : the processing web services, as docker images, created by pesto to expose your algorithm. Contacts Coming soon ...","title":"About"},{"location":"index.html#pesto-processing-factory","text":"PESTO is a packaging tool inspired from pip, maven and similar dependencies managers. It contains shell tools to generate all the boiler plate to build an OpenAPI processing web service compliant with the Geoprocessing-API . PESTO is designed to ease the process of packaging a Python algorithm as a processing web service into a docker image. The deployment of a web service becomes now as easy as filling few configuration files. PESTO is composed of the following components: PESTO-CLI : the command line interface used to create a pesto project and package processing algorithms. PESTO-PROJ : the project workspace created by PESTO to configure the packaging process of your algorithm. PESTO-PWS : the processing web services, as docker images, created by pesto to expose your algorithm.","title":"PESTO : ProcESsing facTOry"},{"location":"index.html#contacts","text":"Coming soon ...","title":"Contacts"},{"location":"about.html","text":"","title":"About"},{"location":"configuration.html","text":"Configuration of your PESTO web service PESTO configuration is done in two steps : edit each json configuration file in pesto/api directory, call your algorithm library from the algorithm/process.py entry point. Note: Always start from the pesto-template as it is already a working PESTO project. Basic usage Input / Output json validation The first thing to do is define the processing input and output. The REST API use json to communicate with external services or users. We then use JSON schema to validate input payloads. pesto/api/input_schema.json : specify the input validation schema, pesto/api/output_schema.json : specify the output validation schema. Requirements PESTO provides a generic way to include any files in the final docker image using the pesto/build/requirements.json file : { \"environments\": { \"DEEPWORK\": \"/deep/deliveries\", \"DEEPDELIVERY\": \"/deep/deliveries\" }, \"requirements\": { \"lib1\": { \"from\": \"file:///tmp/my-lib1.tar.gz\", \"to\": \"/opt/lib1\", \"type\": \"python\" }, \"lib2\": { \"from\": \"file:///tmp/my-lib2.tar.gz\", \"to\": \"/opt/lib2\", \"type\": \"pip\" }, \"model\": { \"from\": \"gs://path/to/my-model.tar.gz\", \"to\": \"/opt/model\" } } } The following fields are required : dockerBaseImage : the docker image to use as a base, environments: some user defined variables, requirements: A (from,to) list, where 'from' is an URI to some files and 'to' is the target path in the docker image. Based on the \"type\" attribute, pesto will : - python : add the \"to\" path to the PYTHONPATH, - pip : pip install the wheel archive, - default : simply copy the files (uncompressed the archive). Compressed files: Pesto suport 'tar.gz' archives. The files are always uncompressed (if needed) before being copied. Advanced: Each requirement accepts an optional 'type' field : python : will add the 'to' path to the PYTHON_PATH environment variable, pip : will run a 'pip install' command on the provided wheel or setuptools compatible 'tar.gz' archive. Python entry point The algorithm/process.py should contains a 'Process' class with the 'Process.process()' method. Be sure to match the input_schema.json and output_schema.json in the 'Process.process()' method arguments and result. Important: Images are converted to/from numpy arrays by PESTO. Thus, the 'Process.process()' method should expect to receive numpy arrays and always return images as numpy arrays. class Process(object): def process(self, image, dict_parameter): # do some processing on the input return { 'partial_result_1': {}, 'partial_result_2': {}, 'confidence': 1.0 } Advanced usage Profile specific implementation PESTO provides some helper libraries (pesto-common) to detect which profile was used during packaging. If multiple version of an algorithm must be maintained, it is advised to : create multiple python implementation of the 'Process' class (one per file), use the pesto_common.pesto_util.is_profile_active() to check the current profile, select the proper implementation of 'Process' to import. The following is an example using pesto profiles to switch from one algo to another at build time. from pesto.common.pesto import Pesto if Pesto.is_profile_active('v1'): from algorithm import process_v1 Process = process_v1.Process elif Pesto.is_profile_active('v2'): from algorithm import process_v2 Process = process_v2.Process else: from algorithm import process_v3 Process = process_v3.Process","title":"Configuration"},{"location":"configuration.html#configuration-of-your-pesto-web-service","text":"PESTO configuration is done in two steps : edit each json configuration file in pesto/api directory, call your algorithm library from the algorithm/process.py entry point. Note: Always start from the pesto-template as it is already a working PESTO project.","title":"Configuration of your PESTO web service"},{"location":"configuration.html#basic-usage","text":"","title":"Basic usage"},{"location":"configuration.html#input-output-json-validation","text":"The first thing to do is define the processing input and output. The REST API use json to communicate with external services or users. We then use JSON schema to validate input payloads. pesto/api/input_schema.json : specify the input validation schema, pesto/api/output_schema.json : specify the output validation schema.","title":"Input / Output json validation"},{"location":"configuration.html#requirements","text":"PESTO provides a generic way to include any files in the final docker image using the pesto/build/requirements.json file : { \"environments\": { \"DEEPWORK\": \"/deep/deliveries\", \"DEEPDELIVERY\": \"/deep/deliveries\" }, \"requirements\": { \"lib1\": { \"from\": \"file:///tmp/my-lib1.tar.gz\", \"to\": \"/opt/lib1\", \"type\": \"python\" }, \"lib2\": { \"from\": \"file:///tmp/my-lib2.tar.gz\", \"to\": \"/opt/lib2\", \"type\": \"pip\" }, \"model\": { \"from\": \"gs://path/to/my-model.tar.gz\", \"to\": \"/opt/model\" } } } The following fields are required : dockerBaseImage : the docker image to use as a base, environments: some user defined variables, requirements: A (from,to) list, where 'from' is an URI to some files and 'to' is the target path in the docker image. Based on the \"type\" attribute, pesto will : - python : add the \"to\" path to the PYTHONPATH, - pip : pip install the wheel archive, - default : simply copy the files (uncompressed the archive). Compressed files: Pesto suport 'tar.gz' archives. The files are always uncompressed (if needed) before being copied. Advanced: Each requirement accepts an optional 'type' field : python : will add the 'to' path to the PYTHON_PATH environment variable, pip : will run a 'pip install' command on the provided wheel or setuptools compatible 'tar.gz' archive.","title":"Requirements"},{"location":"configuration.html#python-entry-point","text":"The algorithm/process.py should contains a 'Process' class with the 'Process.process()' method. Be sure to match the input_schema.json and output_schema.json in the 'Process.process()' method arguments and result. Important: Images are converted to/from numpy arrays by PESTO. Thus, the 'Process.process()' method should expect to receive numpy arrays and always return images as numpy arrays. class Process(object): def process(self, image, dict_parameter): # do some processing on the input return { 'partial_result_1': {}, 'partial_result_2': {}, 'confidence': 1.0 }","title":"Python entry point"},{"location":"configuration.html#advanced-usage","text":"","title":"Advanced usage"},{"location":"configuration.html#profile-specific-implementation","text":"PESTO provides some helper libraries (pesto-common) to detect which profile was used during packaging. If multiple version of an algorithm must be maintained, it is advised to : create multiple python implementation of the 'Process' class (one per file), use the pesto_common.pesto_util.is_profile_active() to check the current profile, select the proper implementation of 'Process' to import. The following is an example using pesto profiles to switch from one algo to another at build time. from pesto.common.pesto import Pesto if Pesto.is_profile_active('v1'): from algorithm import process_v1 Process = process_v1.Process elif Pesto.is_profile_active('v2'): from algorithm import process_v2 Process = process_v2.Process else: from algorithm import process_v3 Process = process_v3.Process","title":"Profile specific implementation"},{"location":"conventions.html","text":"Conventions: using PESTO properly Image format (numpy array) PESTO is based on numpy arrays to send to or receive from the packaged algorithm. The convention is to encode images as arrays with 3 dimensions (C,H,W) : C is the channel number H is the lines number W is the columns number For example, an RGBA image of dimension 256x256 should be encoded as a numpy array of shape (4,256,256). Defining requirements PESTO can handle requirements in many format (using the 'pesto/build/requirements.json' file) : wheel : with 'python setup.py bdist_wheel' on your project and with type 'pip' in 'requirements.json' tar.gz : 'python setup.py sdist' on your project and with type 'pip' in 'requirements.json' tar.gz : with no type defined, the archive will just be unpacked in the docker image tar.gz : (DEPRECATED) with type 'python', the archive will be unpacked and its path added to the PYTHON_PATH. Warning : The tar.gz with type 'python' usage is DEPRECATED and will fail with an archive build with setuptools. Such an archive contains a root folder that should be removed when adding the path to PYTHON_PATH. Automatic Integration Test PESTO helps you automate testing : create a 'test' folder in '{pesto_project_path}/pesto/tests/features/reosurces/' directory, add one file per entry in your expected input (cf. 'pesto/api/input_schema.json'), deploy your PESTO project : pesto deploy {pesto_project_path} if the test fails : read the instructions to make the test pass ! Cascading profiles : reusable configuration PESTO supports multiple configurations files organized in profiles . Delivery name convention Given a build.json file : { \"name\": \"service-xxx\", \"version\": \"a.b.c\" } and the build command : pesto build build.json -p p1 p2 The packaged docker image is automatically named : service-xxx:a.b.c-p1-p2 Docker image naming Docker images naming convention is : { service-name }:{version} when no profile is used { service-name }:{version}-stateful when no profile is used and the service is asynchronous. { service-name }:{version}-{profile} when a profile is specified { service-name }:{version}-{profile}-stateful when a profile is specified and the service is asynchronous.","title":"Conventions"},{"location":"conventions.html#conventions-using-pesto-properly","text":"","title":"Conventions: using PESTO properly"},{"location":"conventions.html#image-format-numpy-array","text":"PESTO is based on numpy arrays to send to or receive from the packaged algorithm. The convention is to encode images as arrays with 3 dimensions (C,H,W) : C is the channel number H is the lines number W is the columns number For example, an RGBA image of dimension 256x256 should be encoded as a numpy array of shape (4,256,256).","title":"Image format (numpy array)"},{"location":"conventions.html#defining-requirements","text":"PESTO can handle requirements in many format (using the 'pesto/build/requirements.json' file) : wheel : with 'python setup.py bdist_wheel' on your project and with type 'pip' in 'requirements.json' tar.gz : 'python setup.py sdist' on your project and with type 'pip' in 'requirements.json' tar.gz : with no type defined, the archive will just be unpacked in the docker image tar.gz : (DEPRECATED) with type 'python', the archive will be unpacked and its path added to the PYTHON_PATH. Warning : The tar.gz with type 'python' usage is DEPRECATED and will fail with an archive build with setuptools. Such an archive contains a root folder that should be removed when adding the path to PYTHON_PATH.","title":"Defining requirements"},{"location":"conventions.html#automatic-integration-test","text":"PESTO helps you automate testing : create a 'test' folder in '{pesto_project_path}/pesto/tests/features/reosurces/' directory, add one file per entry in your expected input (cf. 'pesto/api/input_schema.json'), deploy your PESTO project : pesto deploy {pesto_project_path} if the test fails : read the instructions to make the test pass !","title":"Automatic Integration Test"},{"location":"conventions.html#cascading-profiles-reusable-configuration","text":"PESTO supports multiple configurations files organized in profiles .","title":"Cascading profiles : reusable configuration"},{"location":"conventions.html#delivery-name-convention","text":"Given a build.json file : { \"name\": \"service-xxx\", \"version\": \"a.b.c\" } and the build command : pesto build build.json -p p1 p2 The packaged docker image is automatically named : service-xxx:a.b.c-p1-p2","title":"Delivery name convention"},{"location":"conventions.html#docker-image-naming","text":"Docker images naming convention is : { service-name }:{version} when no profile is used { service-name }:{version}-stateful when no profile is used and the service is asynchronous. { service-name }:{version}-{profile} when a profile is specified { service-name }:{version}-{profile}-stateful when a profile is specified and the service is asynchronous.","title":"Docker image naming"},{"location":"cookbook.html","text":"PESTO Cookbook How to load resources only once (at server start) : I don't want to reload my model for each prediction ? Use static variables in Process class. Ex: class Process: heavy_requirement = None def process(self, *args, **kwargs): # load only once if Process.heavy_requirement is None: Process.heavy_requirement = ... # use heavy_requirement for processing ... How to use profiles to factorize PESTO configurations ? Imagine you want to build two services out of one algorithm implementation : cpu : run on CPU with a specific model and requirements, gpu : run on GPU with another model and requirements. In 'pesto/build/' you can use three files to define requirements : requirements.json : common requirements, requirements.cpu.json : specific CPU requirements, requirements.gpu.json : specific GPU requirements. Then build your service with the '--profile' or '-p' option : pesto build path/to/project -p cpu pesto build path/to/project -p gpu More details in the profile section of the PESTO documentation. How to use profiles to build variants of a same algorithm ? For exemple you want to build your algorithm with 2 variants : raster : return detections as an image mask, vector : return detections as a geometry. Build your service with the '--profile' or '-p' option : pesto build path/to/project -p raster cpu pesto build path/to/project -p vector cpu Use 'Pesto.is_profile_active(profile:str)' to check at runtime which profile was used during build. from pesto.common.pesto import Pesto class Process: def process(self, *args, **kwargs): mask_output = ... if Pesto.is_profile_active('raster'): return mask_output if Pesto.is_profile_active('vector'): return vectorize(mask_output) raise NotImplementedError() How to install / include files in the docker image ? Pesto will copy or pip install all your requirements in the output docker image. You just need to define all your service requirements in the 'pesto/build/requirements.json' file. How to test the service built with PESTO ? Be sure to have a proper pesto-service python project (use the pesto init command). Then, go in your project pesto/tests directory and start editing files. The pesto/tests is composed of : - some directories (one per processing to be run), - a expected_describe.json file. Each pesto/tests/xxx directory is composed of : - an input directory matching pesto/api/input_schema.json , - an output directory matching pesto/api/output_schema.json . The input and output directories both describes a json payload (the processing input and output). Each filename key.type in those folders must match an entry in its corresponding *_schema.json : - key is the key in the *_schema.json , - type is the primitive type of the key : - string, float, int, - json : dictionaries, - .tif, .jpg, *.png for images. - arrays can be constructed using a folder key containing its enumerted items ( 1.int , 2.int , ...) ex: The following describes the correspondance between the file structure and the json payload. pesto/tests/input key1.string (containing text ) key2.int (containing 33 ) key3.float (containing 3.14 ) { \"key1\" : \"text\", \"key2\" : 33, \"key3\" : 3.14 } More examples are provided in the default pesto template. Then, it is required to build your project (once). pesto build /path/to/pesto-service -p p1 p2 Finally, run the tests : pesto test /path/to/pesto-service -p p1 p2 zero code dans le template-service (donc on peut faire \u00e9voluer \"pesto test\" sans avoir \u00e0 toucher les projets (comme j'ai du le faire cette fois-ci) remplir directement les dossiers \u00e0 la racine de pesto/tests lancer avec pesto tests /path/to/pesto-service -p cpu pour tester une image docker d\u00e9j\u00e0 build\u00e9 avec le profil choisi.","title":"Cookbook"},{"location":"cookbook.html#pesto-cookbook","text":"","title":"PESTO Cookbook"},{"location":"cookbook.html#how-to-load-resources-only-once-at-server-start-i-dont-want-to-reload-my-model-for-each-prediction","text":"Use static variables in Process class. Ex: class Process: heavy_requirement = None def process(self, *args, **kwargs): # load only once if Process.heavy_requirement is None: Process.heavy_requirement = ... # use heavy_requirement for processing ...","title":"How to load resources only once (at server start) : I don't want to reload my model for each prediction ?"},{"location":"cookbook.html#how-to-use-profiles-to-factorize-pesto-configurations","text":"Imagine you want to build two services out of one algorithm implementation : cpu : run on CPU with a specific model and requirements, gpu : run on GPU with another model and requirements. In 'pesto/build/' you can use three files to define requirements : requirements.json : common requirements, requirements.cpu.json : specific CPU requirements, requirements.gpu.json : specific GPU requirements. Then build your service with the '--profile' or '-p' option : pesto build path/to/project -p cpu pesto build path/to/project -p gpu More details in the profile section of the PESTO documentation.","title":"How to use profiles to factorize PESTO configurations ?"},{"location":"cookbook.html#how-to-use-profiles-to-build-variants-of-a-same-algorithm","text":"For exemple you want to build your algorithm with 2 variants : raster : return detections as an image mask, vector : return detections as a geometry. Build your service with the '--profile' or '-p' option : pesto build path/to/project -p raster cpu pesto build path/to/project -p vector cpu Use 'Pesto.is_profile_active(profile:str)' to check at runtime which profile was used during build. from pesto.common.pesto import Pesto class Process: def process(self, *args, **kwargs): mask_output = ... if Pesto.is_profile_active('raster'): return mask_output if Pesto.is_profile_active('vector'): return vectorize(mask_output) raise NotImplementedError()","title":"How to use profiles to build variants of a same algorithm ?"},{"location":"cookbook.html#how-to-install-include-files-in-the-docker-image","text":"Pesto will copy or pip install all your requirements in the output docker image. You just need to define all your service requirements in the 'pesto/build/requirements.json' file.","title":"How to install / include files in the docker image ?"},{"location":"cookbook.html#how-to-test-the-service-built-with-pesto","text":"Be sure to have a proper pesto-service python project (use the pesto init command). Then, go in your project pesto/tests directory and start editing files. The pesto/tests is composed of : - some directories (one per processing to be run), - a expected_describe.json file. Each pesto/tests/xxx directory is composed of : - an input directory matching pesto/api/input_schema.json , - an output directory matching pesto/api/output_schema.json . The input and output directories both describes a json payload (the processing input and output). Each filename key.type in those folders must match an entry in its corresponding *_schema.json : - key is the key in the *_schema.json , - type is the primitive type of the key : - string, float, int, - json : dictionaries, - .tif, .jpg, *.png for images. - arrays can be constructed using a folder key containing its enumerted items ( 1.int , 2.int , ...) ex: The following describes the correspondance between the file structure and the json payload. pesto/tests/input key1.string (containing text ) key2.int (containing 33 ) key3.float (containing 3.14 ) { \"key1\" : \"text\", \"key2\" : 33, \"key3\" : 3.14 } More examples are provided in the default pesto template. Then, it is required to build your project (once). pesto build /path/to/pesto-service -p p1 p2 Finally, run the tests : pesto test /path/to/pesto-service -p p1 p2 zero code dans le template-service (donc on peut faire \u00e9voluer \"pesto test\" sans avoir \u00e0 toucher les projets (comme j'ai du le faire cette fois-ci) remplir directement les dossiers \u00e0 la racine de pesto/tests lancer avec pesto tests /path/to/pesto-service -p cpu pour tester une image docker d\u00e9j\u00e0 build\u00e9 avec le profil choisi.","title":"How to test the service built with PESTO ?"},{"location":"design.html","text":"Design presentation Introduction PESTO stands for ProcESsing facTOry and was initially designed to answer the need for fast and efficient integration of image processing algorithms using deep learning (DL) in our ground segment products. Next sections are organized as follows: Need identification: this section presents the expectations, what we would like to do. Base architecture identification: this section details the architecture of Pesto. It describes how we answer to our objectives. Processing API management: this section provides guidelines on how to design a processing API for PESTO. Performances: this section tells how PESTO should perform, how we demonstrated it was scalable. Who uses PESTO: this section provides a list of projects / products that are using PESTO. Need identification The road from the design and development of a DL algorithm by data scientists to their integration in the image chain and finally the ground segment can be fastidious due to all the teams and all the required skills involved along the integration chain. Moreover, a DL solution, like cloud detection for instance, will usually be included in different products of our offer (like PNEO, DIGINEO or OneAtlas) having their own integration technologies, constraints and specificities. In addition, DL algorithms are frequently updated, either when the training dataset or the model is modified. Therefore, we can\u2019t afford to have an integration effort too long and complex, for each algorithm in each of our product. It is then clear that standardizing key APIs, defining a common process and sharing common tools could greatly facilitate the delivery of DL algorithms and by extension of data processing algorithms, either delivered by our providers or designed by ourselves. From these remarks and an analysis of what should be PESTO, a base architecture is proposed in the next section. Base architecture definition Let\u2019s take the widespread use case of cloud detection on satellite imagery in order to introduce the main integration steps and possibilities. A classical workflow applied when images are received is presented in Figure Cloud Detection Pipeline Figure: Illustration of a cloud detection pipeline As we see, the cloud detection algorithm is integrated in a global image processing chain that will trigger many instances of it. Indeed, large scene images has to be split in smaller images that are compatible with the algorithm (RAM or GPU memory limit), and the processing needs to be distributed to provide high processing throughput and low latency. PESTO should then facilitate the integration of processing algorithms in various distributed processing frameworks used in our products. In the current illustration, the algorithm processes images as they are received, but it might also be on user request for on-demand processing or based on an event triggered by a monitoring function in other scenario. PESTO should then facilitate the reuse of processing algorithms by various orchestrators used in our products. The described pipeline does not detail the type and format of the data that is exchanged in between tasks. For image tiles, it could be a binary buffer containing the image itself or an URL pointing to a file to be fetched. Moreover, the information could be provided by different technologies (HTTP/REST, Apache KAFKA bus, SOAP,...). It will depend on the solution selected at the system level. But in any manner, the processing algorithm will need the image data. PESTO should then facilitate the definition of algorithms API and encourage the definition and use of common data types. And as it was mentioned several times, the processing algorithm needs to be deployed in many places. Therefore, PESTO should allow implementing the strategy develop and validate once, deploy everywhere . The base architecture of PESTO is then derived from these needs. PESTO is composed of a runtime proper to each processing function, a packaging manager to create PESTO runtimes tuned for each processing function and a testing framework. PESTO runtime The runtime is a webserver which conforms to an OpenAPI specification. The OpenAPI Specification (OAS) defines a standard, language-agnostic interface to RESTful APIs which allows both humans and computers to discover and understand the capabilities of the service without access to source code, documentation, or through network traffic inspection. When properly defined, a consumer can understand and interact with the remote service with a minimal amount of implementation logic, see SWAGGER . The architecture of the runtime is presented in Figure PESTO instance for cloud detection Figure: Architecture of a PESTO runtime for cloud detection PESTO web server offers three web services that are : - /api/v1/health: it provides information on the availability of the service, - /api/v1/describe: it provides information on the processing service that is deployed. Information on inputs, outputs, deployment requirements and so on are given here. PESTO does not constrain the definition of input and output parameters to offer a versatile solution. In return, a standardization effort has to be conducted in parallel. - /api/v1/process: call the processing function. Request parameters must respect the definition provided by /api/v1/describe. PESTO is also in charge to convert, when required, the data from the REST API to the processing API. For most of parameters, they won\u2019t be any major conversion. In the current implementation, only the image type is converted such that the processing has access to the image data as a buffer. The philosophy is to remove any I/O operations from the algorithm implementation to limit the any adherence with the data source or destination. The runtime is provided as a standalone docker image that can be deployed everywhere and how many times as necessary. PESTO packaging manager The packaging manager is a set of tools and templates to help the data scientist to deliver its algorithm. Indeed, one goal of PESTO is really to ease the delivery of an algorithm designed by a data scientist. The packaging workflow is described in Figure PESTO packaging workflow Figure: PESTO packaging workflow PESTO testing framework PESTO provides mechanisms to test the embedded processing function. A set of input and expected output parameters are provided by the data scientist. Then, the test function deploys the PESTO runtime and run tests. Implementation details PESTO is developed with Python3. Indeed, many Deep Leaning tools and frameworks are provided in this language. Moreover, Python can easily use native code if necessary. SANIC is used to implement the webserver. It offers async operations which allow to write non-blocking code. In order to make the link between requests on the web server and the processing implemented by the data scientist, a template to implement a python package and to provide a description of the API as well as build requirements is provided. The python package is named algorithm.process.Process. The data scientist needs to fill the signature and the content of the process function and in particular include the call to the implementation of the processing function. Then a set of configuration files have to be updated. A first set of configuration files (located in pesto/api) defines the API of the algorithm. Their content is described in the next section. A second set of configuration files (located in pesto/build) defines how to build the runtime. We can define which root docker image to use, how to include DL models and parameters in the runtime, the name and version of the runtime,... One additional feature is the possibility to define several independent runtime profiles. For example, it is useful to define a runtime using only CPUs and a runtime using the GPU. The process is depicted in Figure PESTO packaging workflow Figure: PESTO packaging workflow One last implementation detail that is worth to mention, is the management of the image type. The REST API allows to pass an image either as a string designating an image file (either on a web server, on the disk, or in a GCP storage) or an image raw buffer encoded in base64. PESTO is then in charge to fetch the data and to provide it to the algorithm. Processing API management The PESTO runtime is designed to be extremely versatile by letting a full freedom on the definition of interfaces. The solution can then address many, if not all, use cases, which makes it its strength. PESTO does not guarantee the interoperability of processing functions which is also a key element in our ground segment solutions. Therefore, a complementary effort is conducted to standardize common data types in order to enforce interoperability. PESTO recommended philosophy is then to capitalize JSON Schema defining data types as well as functions used to convert from the REST API type to the algorithm type (like it is done for images). This effort has been conducted for Earth Observation data processing jointly with Airbus Defense and Space / Connected Intelligence teams. A common API, named geoprocessing-api, was then defined and can be found here . We provide here some guidelines to define and manage JSON Schema used to define API. Define common data types and rely on existing ones as much as possible to ease interoperability, Define data types that can be managed by higher level orchestrators: they need to known data types to provide the right information in the right format, possibly calling converters, Define API that will allow to chain processing function. The definition of processing API should then be organized as depicted in Figure Processing API management Figure: Processing API management MyDescribe.yml: ** Declares features of the process. It is used by orchestrators to select a service, deploy its docker image, feed it with the right data and exploit its result, ** The Format is extremely open to support any type of micro-service, however it is already batch processing compliant. Default data types for GeoApplications: ** image.json: Declares the format of an image and its attributes: compression type, spectral bands, sensor, ephemeris. ** imageCapabilities: Declares how to specify filters describing supported images. ** inputCapabilities: gather together all possible input capabilities of the project, somehow it references ImageCapabilities.json Performances PESTO runtime is a wrapper around the algorithm to make it available as a web service. It transfers inputs and outputs from one side to the other. The intrinsic performances, i.e. performances related to the goal of the function like the detection rate for object detection, of the algorithm are not altered by PESTO. It can eventually add a little processing time or latency overhead, i.e. extrinsic performances. But PESTO should not affect them in a significative way as it simply passes REST API arguments to the algorithm. It should provide equivalent overhead on extrinsic performances as any web service implementation. The memory overhead, that also characterizes the extrinsic performances, has not been evaluated yet. But in our use cases involving image processing, it should remain negligible. PESTO has been deployed at scale in Google Cloud Platform to process large satellite imagery. The architecture of the deployed system is presented in Figure PESTO at scale for EO processing with GPU . PESTO runtimes are identified with the icon . The platform was created with 3 GPU workers and 13 workers (with 8 CPU each) for PESTO runtimes. Figure: PESTO at scale for EO processing with GPU PESTO has also been deployed at larger scale with Conductor (Orchestrator from Netflix) to run a pipeline of two tasks provided as PESTO micro services, see Figure PESTO at scale with Conductor . 100 workers for PESTO were created. Figure: PESTO at scale with Conductor Who uses PESTO PESTO is currently used by Airbus Defense and Space / Connected Intelligence. PESTO is under integration by the Airbus Defense and Space / Space Systems Oasis project. It is used to integrate and manage algorithms for TM/TC analysis. PESTO is under integration by Airbus Defense and Space / Space Systems in its ground segment image chain. Workshops have been taking place with end-users in order define a solution that first could be integrated in our ground segment product lines. They have contributed to the definition of the product and have validated its integration in their own software stack. PESTO roadmap PESTO was thought to accelerate the integration in our products of Deep Learning algorithms, being developed by Airbus or by partners. That is why, one of the first priorities is to make it open source. PESTO was initially designed for image processing in mind. Some specificities were then included in the runtime. Our second priority is to set up mechanisms to include user data converters as plugins and to support various Linux families. PESTO needs to be orchestrated. Simple tutorials and examples to deploy PESTO at scale and manage distributed processing have to be written in order to foster its acceptance by the community. PESTO testing and deployment framework still need additional effort to offer a complete workflow towards continuous deployment and validation.","title":"Design"},{"location":"design.html#design-presentation","text":"","title":"Design presentation"},{"location":"design.html#introduction","text":"PESTO stands for ProcESsing facTOry and was initially designed to answer the need for fast and efficient integration of image processing algorithms using deep learning (DL) in our ground segment products. Next sections are organized as follows: Need identification: this section presents the expectations, what we would like to do. Base architecture identification: this section details the architecture of Pesto. It describes how we answer to our objectives. Processing API management: this section provides guidelines on how to design a processing API for PESTO. Performances: this section tells how PESTO should perform, how we demonstrated it was scalable. Who uses PESTO: this section provides a list of projects / products that are using PESTO.","title":"Introduction"},{"location":"design.html#need-identification","text":"The road from the design and development of a DL algorithm by data scientists to their integration in the image chain and finally the ground segment can be fastidious due to all the teams and all the required skills involved along the integration chain. Moreover, a DL solution, like cloud detection for instance, will usually be included in different products of our offer (like PNEO, DIGINEO or OneAtlas) having their own integration technologies, constraints and specificities. In addition, DL algorithms are frequently updated, either when the training dataset or the model is modified. Therefore, we can\u2019t afford to have an integration effort too long and complex, for each algorithm in each of our product. It is then clear that standardizing key APIs, defining a common process and sharing common tools could greatly facilitate the delivery of DL algorithms and by extension of data processing algorithms, either delivered by our providers or designed by ourselves. From these remarks and an analysis of what should be PESTO, a base architecture is proposed in the next section.","title":"Need identification"},{"location":"design.html#base-architecture-definition","text":"Let\u2019s take the widespread use case of cloud detection on satellite imagery in order to introduce the main integration steps and possibilities. A classical workflow applied when images are received is presented in Figure Cloud Detection Pipeline Figure: Illustration of a cloud detection pipeline As we see, the cloud detection algorithm is integrated in a global image processing chain that will trigger many instances of it. Indeed, large scene images has to be split in smaller images that are compatible with the algorithm (RAM or GPU memory limit), and the processing needs to be distributed to provide high processing throughput and low latency. PESTO should then facilitate the integration of processing algorithms in various distributed processing frameworks used in our products. In the current illustration, the algorithm processes images as they are received, but it might also be on user request for on-demand processing or based on an event triggered by a monitoring function in other scenario. PESTO should then facilitate the reuse of processing algorithms by various orchestrators used in our products. The described pipeline does not detail the type and format of the data that is exchanged in between tasks. For image tiles, it could be a binary buffer containing the image itself or an URL pointing to a file to be fetched. Moreover, the information could be provided by different technologies (HTTP/REST, Apache KAFKA bus, SOAP,...). It will depend on the solution selected at the system level. But in any manner, the processing algorithm will need the image data. PESTO should then facilitate the definition of algorithms API and encourage the definition and use of common data types. And as it was mentioned several times, the processing algorithm needs to be deployed in many places. Therefore, PESTO should allow implementing the strategy develop and validate once, deploy everywhere . The base architecture of PESTO is then derived from these needs. PESTO is composed of a runtime proper to each processing function, a packaging manager to create PESTO runtimes tuned for each processing function and a testing framework.","title":"Base architecture definition"},{"location":"design.html#pesto-runtime","text":"The runtime is a webserver which conforms to an OpenAPI specification. The OpenAPI Specification (OAS) defines a standard, language-agnostic interface to RESTful APIs which allows both humans and computers to discover and understand the capabilities of the service without access to source code, documentation, or through network traffic inspection. When properly defined, a consumer can understand and interact with the remote service with a minimal amount of implementation logic, see SWAGGER . The architecture of the runtime is presented in Figure PESTO instance for cloud detection Figure: Architecture of a PESTO runtime for cloud detection PESTO web server offers three web services that are : - /api/v1/health: it provides information on the availability of the service, - /api/v1/describe: it provides information on the processing service that is deployed. Information on inputs, outputs, deployment requirements and so on are given here. PESTO does not constrain the definition of input and output parameters to offer a versatile solution. In return, a standardization effort has to be conducted in parallel. - /api/v1/process: call the processing function. Request parameters must respect the definition provided by /api/v1/describe. PESTO is also in charge to convert, when required, the data from the REST API to the processing API. For most of parameters, they won\u2019t be any major conversion. In the current implementation, only the image type is converted such that the processing has access to the image data as a buffer. The philosophy is to remove any I/O operations from the algorithm implementation to limit the any adherence with the data source or destination. The runtime is provided as a standalone docker image that can be deployed everywhere and how many times as necessary.","title":"PESTO runtime"},{"location":"design.html#pesto-packaging-manager","text":"The packaging manager is a set of tools and templates to help the data scientist to deliver its algorithm. Indeed, one goal of PESTO is really to ease the delivery of an algorithm designed by a data scientist. The packaging workflow is described in Figure PESTO packaging workflow Figure: PESTO packaging workflow","title":"PESTO packaging manager"},{"location":"design.html#pesto-testing-framework","text":"PESTO provides mechanisms to test the embedded processing function. A set of input and expected output parameters are provided by the data scientist. Then, the test function deploys the PESTO runtime and run tests.","title":"PESTO testing framework"},{"location":"design.html#implementation-details","text":"PESTO is developed with Python3. Indeed, many Deep Leaning tools and frameworks are provided in this language. Moreover, Python can easily use native code if necessary. SANIC is used to implement the webserver. It offers async operations which allow to write non-blocking code. In order to make the link between requests on the web server and the processing implemented by the data scientist, a template to implement a python package and to provide a description of the API as well as build requirements is provided. The python package is named algorithm.process.Process. The data scientist needs to fill the signature and the content of the process function and in particular include the call to the implementation of the processing function. Then a set of configuration files have to be updated. A first set of configuration files (located in pesto/api) defines the API of the algorithm. Their content is described in the next section. A second set of configuration files (located in pesto/build) defines how to build the runtime. We can define which root docker image to use, how to include DL models and parameters in the runtime, the name and version of the runtime,... One additional feature is the possibility to define several independent runtime profiles. For example, it is useful to define a runtime using only CPUs and a runtime using the GPU. The process is depicted in Figure PESTO packaging workflow Figure: PESTO packaging workflow One last implementation detail that is worth to mention, is the management of the image type. The REST API allows to pass an image either as a string designating an image file (either on a web server, on the disk, or in a GCP storage) or an image raw buffer encoded in base64. PESTO is then in charge to fetch the data and to provide it to the algorithm.","title":"Implementation details"},{"location":"design.html#processing-api-management","text":"The PESTO runtime is designed to be extremely versatile by letting a full freedom on the definition of interfaces. The solution can then address many, if not all, use cases, which makes it its strength. PESTO does not guarantee the interoperability of processing functions which is also a key element in our ground segment solutions. Therefore, a complementary effort is conducted to standardize common data types in order to enforce interoperability. PESTO recommended philosophy is then to capitalize JSON Schema defining data types as well as functions used to convert from the REST API type to the algorithm type (like it is done for images). This effort has been conducted for Earth Observation data processing jointly with Airbus Defense and Space / Connected Intelligence teams. A common API, named geoprocessing-api, was then defined and can be found here . We provide here some guidelines to define and manage JSON Schema used to define API. Define common data types and rely on existing ones as much as possible to ease interoperability, Define data types that can be managed by higher level orchestrators: they need to known data types to provide the right information in the right format, possibly calling converters, Define API that will allow to chain processing function. The definition of processing API should then be organized as depicted in Figure Processing API management Figure: Processing API management MyDescribe.yml: ** Declares features of the process. It is used by orchestrators to select a service, deploy its docker image, feed it with the right data and exploit its result, ** The Format is extremely open to support any type of micro-service, however it is already batch processing compliant. Default data types for GeoApplications: ** image.json: Declares the format of an image and its attributes: compression type, spectral bands, sensor, ephemeris. ** imageCapabilities: Declares how to specify filters describing supported images. ** inputCapabilities: gather together all possible input capabilities of the project, somehow it references ImageCapabilities.json","title":"Processing API management"},{"location":"design.html#performances","text":"PESTO runtime is a wrapper around the algorithm to make it available as a web service. It transfers inputs and outputs from one side to the other. The intrinsic performances, i.e. performances related to the goal of the function like the detection rate for object detection, of the algorithm are not altered by PESTO. It can eventually add a little processing time or latency overhead, i.e. extrinsic performances. But PESTO should not affect them in a significative way as it simply passes REST API arguments to the algorithm. It should provide equivalent overhead on extrinsic performances as any web service implementation. The memory overhead, that also characterizes the extrinsic performances, has not been evaluated yet. But in our use cases involving image processing, it should remain negligible. PESTO has been deployed at scale in Google Cloud Platform to process large satellite imagery. The architecture of the deployed system is presented in Figure PESTO at scale for EO processing with GPU . PESTO runtimes are identified with the icon . The platform was created with 3 GPU workers and 13 workers (with 8 CPU each) for PESTO runtimes. Figure: PESTO at scale for EO processing with GPU PESTO has also been deployed at larger scale with Conductor (Orchestrator from Netflix) to run a pipeline of two tasks provided as PESTO micro services, see Figure PESTO at scale with Conductor . 100 workers for PESTO were created. Figure: PESTO at scale with Conductor","title":"Performances"},{"location":"design.html#who-uses-pesto","text":"PESTO is currently used by Airbus Defense and Space / Connected Intelligence. PESTO is under integration by the Airbus Defense and Space / Space Systems Oasis project. It is used to integrate and manage algorithms for TM/TC analysis. PESTO is under integration by Airbus Defense and Space / Space Systems in its ground segment image chain. Workshops have been taking place with end-users in order define a solution that first could be integrated in our ground segment product lines. They have contributed to the definition of the product and have validated its integration in their own software stack.","title":"Who uses PESTO"},{"location":"design.html#pesto-roadmap","text":"PESTO was thought to accelerate the integration in our products of Deep Learning algorithms, being developed by Airbus or by partners. That is why, one of the first priorities is to make it open source. PESTO was initially designed for image processing in mind. Some specificities were then included in the runtime. Our second priority is to set up mechanisms to include user data converters as plugins and to support various Linux families. PESTO needs to be orchestrated. Simple tutorials and examples to deploy PESTO at scale and manage distributed processing have to be written in order to foster its acceptance by the community. PESTO testing and deployment framework still need additional effort to offer a complete workflow towards continuous deployment and validation.","title":"PESTO roadmap"},{"location":"endpoints.html","text":"PESTO endpoints PESTO create web-services following the OpenAPI convention. The webservice is by default available on the port 8080 (mapped to the port 4000 in the project) and offers the endpoints presented here. You can easily launch you web service as follows : {project}/algo_service/scripts/start_service.py describe It provides a full description of the webservice : Required ressources for deployment, Inputs, Outputs, Endpoints -- describe . http://localhost:4000/api/v1/describe process It calls the processing function on provided input parameters. If asynchronous is false in the description, then the call blocks until the result is available. Otherwise, the function returns a jobid for later retrieval of the result -- process http://localhost:4000/api/v1/process health It provides a simple health check -- health http://localhost:4000/api/v1/health Get the status of a submitted asynchronous job This endpoint returns the status of a submitted job. It is available only if asynchronous is set to true in the description. http://localhost:4000/api/v1/jobs/{jobID}/status Get the result of a submitted asynchronous job This endpoint returns the result of a submitted job. It is available only if asynchronous is set to true in the description. http://localhost:4000/api/v1/jobs/{jobID}/results","title":"Web-service"},{"location":"endpoints.html#pesto-endpoints","text":"PESTO create web-services following the OpenAPI convention. The webservice is by default available on the port 8080 (mapped to the port 4000 in the project) and offers the endpoints presented here. You can easily launch you web service as follows : {project}/algo_service/scripts/start_service.py","title":"PESTO endpoints"},{"location":"endpoints.html#describe","text":"It provides a full description of the webservice : Required ressources for deployment, Inputs, Outputs, Endpoints -- describe . http://localhost:4000/api/v1/describe","title":"describe"},{"location":"endpoints.html#process","text":"It calls the processing function on provided input parameters. If asynchronous is false in the description, then the call blocks until the result is available. Otherwise, the function returns a jobid for later retrieval of the result -- process http://localhost:4000/api/v1/process","title":"process"},{"location":"endpoints.html#health","text":"It provides a simple health check -- health http://localhost:4000/api/v1/health","title":"health"},{"location":"endpoints.html#get-the-status-of-a-submitted-asynchronous-job","text":"This endpoint returns the status of a submitted job. It is available only if asynchronous is set to true in the description. http://localhost:4000/api/v1/jobs/{jobID}/status","title":"Get the status of a submitted asynchronous job"},{"location":"endpoints.html#get-the-result-of-a-submitted-asynchronous-job","text":"This endpoint returns the result of a submitted job. It is available only if asynchronous is set to true in the description. http://localhost:4000/api/v1/jobs/{jobID}/results","title":"Get the result of a submitted asynchronous job"},{"location":"installation.html","text":"Get started Get started with PESTO ! Let's see how to package a simple processing algorithm. Prerequisite python >= 3.6 pip docker git Note: You can find in recipes/docker/pesto-tools a Dockerfile to build a PESTO compliant docker image. Installation First, install PESTO-CLI in your environment. The simplest way to do it is as follows: make install Now check your installation. pesto --help Usage PESTO-CLI provides the following commands: init : create a new PESTO-PROJ (i.e. PESTO project), export MY_PESTO_DIR=/tmp/pesto pesto init $MY_PESTO_DIR The created PESTO-PROJ is ready to use with a simple processing. See init for more details on how to tune it for your needs. build : package a PESTO-PWS (create the project docker image containing the default processing web service), pesto build $MY_PESTO_DIR/algo-service See build for more details. test : test a PESTO-PWS (deploy the docker image and run some processings), pesto test $MY_PESTO_DIR/algo-service See test for more details. list : list all pesto workspaces, pesto list You can easily turn on your webservice and check it is running as follows : $MY_PESTO_DIR/algo-service/scripts/start_service.py http://localhost:4000/api/v1/describe The list of endpoints can be found here Conventions : what you need to know The Conventions section gives some (required) hints to PESTO users. PESTO internal workspaces Pesto uses workspaces for building services and storing partial responses in asynchronous mode. Finally, automatic testing copy resources (images) to a temporary folder. Here is a description of the paths where PESTO could write some files : pesto-cli build : $HOME/.pesto/service-name/x.y.z/ pesto-cli build requirements (local cache): $HOME/.pesto/.processing-factory-requirements/ pesto-ws async jobs files : $HOME/.pesto/.processing/jobs/${job_id}/ pesto-template (unit testing) : /tmp/pesto/test","title":"Get started"},{"location":"installation.html#get-started","text":"Get started with PESTO ! Let's see how to package a simple processing algorithm.","title":"Get started"},{"location":"installation.html#prerequisite","text":"python >= 3.6 pip docker git Note: You can find in recipes/docker/pesto-tools a Dockerfile to build a PESTO compliant docker image.","title":"Prerequisite"},{"location":"installation.html#installation","text":"First, install PESTO-CLI in your environment. The simplest way to do it is as follows: make install Now check your installation. pesto --help","title":"Installation"},{"location":"installation.html#usage","text":"PESTO-CLI provides the following commands: init : create a new PESTO-PROJ (i.e. PESTO project), export MY_PESTO_DIR=/tmp/pesto pesto init $MY_PESTO_DIR The created PESTO-PROJ is ready to use with a simple processing. See init for more details on how to tune it for your needs. build : package a PESTO-PWS (create the project docker image containing the default processing web service), pesto build $MY_PESTO_DIR/algo-service See build for more details. test : test a PESTO-PWS (deploy the docker image and run some processings), pesto test $MY_PESTO_DIR/algo-service See test for more details. list : list all pesto workspaces, pesto list You can easily turn on your webservice and check it is running as follows : $MY_PESTO_DIR/algo-service/scripts/start_service.py http://localhost:4000/api/v1/describe The list of endpoints can be found here","title":"Usage"},{"location":"installation.html#conventions-what-you-need-to-know","text":"The Conventions section gives some (required) hints to PESTO users.","title":"Conventions : what you need to know"},{"location":"installation.html#pesto-internal-workspaces","text":"Pesto uses workspaces for building services and storing partial responses in asynchronous mode. Finally, automatic testing copy resources (images) to a temporary folder. Here is a description of the paths where PESTO could write some files : pesto-cli build : $HOME/.pesto/service-name/x.y.z/ pesto-cli build requirements (local cache): $HOME/.pesto/.processing-factory-requirements/ pesto-ws async jobs files : $HOME/.pesto/.processing/jobs/${job_id}/ pesto-template (unit testing) : /tmp/pesto/test","title":"PESTO internal workspaces"},{"location":"pesto_build.html","text":"pesto build : package as a docker image When your project is properly configured (cf. pesto init ), PESTO can build a docker image containing a running web service. Basic usage If your project path is /path/to/your/workspace/xxx-service you can build it using : pesto build /path/to/your/workspace/xxx-service By default, the build command use configuration files in the following directories : service-xxx/pesto/ api build Advanced usage : profiles The PESTO configurations files can be organized by profile. A profile is a 'tag' that can be added between the base name and the extension of any PESTO configuration file. Cascading profiles PESTO can build a same project based on different profiles with the following commands : pesto build /path/to/your/workspace/xxx-service -p p1 p2 The -p p1 p2 option tells PESTO to consider, for each configuration file, the following versions : default : xxx.json (always present) p1 : xxx.p1.json p2 : xxx.p2.json The build is equivalent to have a default xxx.json file build from : xxx.json : initialisation as a dict, xxx.p1.json : add/update at the dict root level, xxx.p2.json : add/update at the dict root level. Warning: Profiles do not work recursively yet. Only the root level is compared. Example: xxx.json { \"base\" : \"default\", \"field_0\" : \"default\" } xxx.p1.json { \"field_0\" : \"profile P1\", \"field_1\" : \"profile P1\" } xxx.p2.json { \"field_0\" : \"profile P2\", \"field_2\" : \"profile P2\" } Will result in the following equivalent xxx.json configuration file : { \"base\" : \"default\", \"field_0\" : \"profile P2\", \"field_1\" : \"profile P1\", \"field_2\" : \"profile P2\" } build.json The pesto/build/build.json file is special : it contains the name and version of the docker image to be built, it does not affect the content of the docker image (only it's name). The build.json file must be explicitly selected, and is not supported by the cascading profile rule : pesto build /path/to/service-xxx/pesto/build/build.p2.json -p p1 p2 List of supported configuration files : The following files are supported by the cascading profiles rule : api description.json input_schema.json output_schema.json output_content.json config_schema.json build build.json config.json requirements.json version.json","title":"build"},{"location":"pesto_build.html#pesto-build-package-as-a-docker-image","text":"When your project is properly configured (cf. pesto init ), PESTO can build a docker image containing a running web service.","title":"pesto build : package as a docker image"},{"location":"pesto_build.html#basic-usage","text":"If your project path is /path/to/your/workspace/xxx-service you can build it using : pesto build /path/to/your/workspace/xxx-service By default, the build command use configuration files in the following directories : service-xxx/pesto/ api build","title":"Basic usage"},{"location":"pesto_build.html#advanced-usage-profiles","text":"The PESTO configurations files can be organized by profile. A profile is a 'tag' that can be added between the base name and the extension of any PESTO configuration file.","title":"Advanced usage : profiles"},{"location":"pesto_build.html#cascading-profiles","text":"PESTO can build a same project based on different profiles with the following commands : pesto build /path/to/your/workspace/xxx-service -p p1 p2 The -p p1 p2 option tells PESTO to consider, for each configuration file, the following versions : default : xxx.json (always present) p1 : xxx.p1.json p2 : xxx.p2.json The build is equivalent to have a default xxx.json file build from : xxx.json : initialisation as a dict, xxx.p1.json : add/update at the dict root level, xxx.p2.json : add/update at the dict root level. Warning: Profiles do not work recursively yet. Only the root level is compared. Example: xxx.json { \"base\" : \"default\", \"field_0\" : \"default\" } xxx.p1.json { \"field_0\" : \"profile P1\", \"field_1\" : \"profile P1\" } xxx.p2.json { \"field_0\" : \"profile P2\", \"field_2\" : \"profile P2\" } Will result in the following equivalent xxx.json configuration file : { \"base\" : \"default\", \"field_0\" : \"profile P2\", \"field_1\" : \"profile P1\", \"field_2\" : \"profile P2\" }","title":"Cascading profiles"},{"location":"pesto_build.html#buildjson","text":"The pesto/build/build.json file is special : it contains the name and version of the docker image to be built, it does not affect the content of the docker image (only it's name). The build.json file must be explicitly selected, and is not supported by the cascading profile rule : pesto build /path/to/service-xxx/pesto/build/build.p2.json -p p1 p2","title":"build.json"},{"location":"pesto_build.html#list-of-supported-configuration-files","text":"The following files are supported by the cascading profiles rule : api description.json input_schema.json output_schema.json output_content.json config_schema.json build build.json config.json requirements.json version.json","title":"List of supported configuration files :"},{"location":"pesto_init.html","text":"pesto init : create a new project The first step to package your processing library is to create a new project. We encourage the following naming convention : given xxx a short name for your processing : then call xxx-lib : the processing library, and call xxx-service : the PESTO packaging project. Basic usage In a terminal : pesto init /path/to/your/workspace This will create a new project named \"/path/to/your/workspace/xxx-service\". The project is ready and setup for a simple processing, but you should at least edit some configuration files to tune PESTO to your needs. define the API pesto/api/input_schema.json : json schema describing the input payloads, pesto/api/output_schema.json : json schema describing the output payloads, implement your process algorithm/process.py : implementation of the processing, package pesto/api/description.json : informative description of the algorithm including deployment requirements, pesto/build/requirements.json : required files and library (your model is defined here) to build the web service. Advanced usage If you have many project sharing some information (your company, email, requirements ...) you can create a specific template. Copy the default template to a new place for your own template : pip show pesto-cli | grep Location | awk '{print $NF}' > /tmp/pesto_site_packages.txt cp -r `cat /tmp/pesto_site_packages.txt`/pesto_cli/resources/pesto-template /path/to/my_pesto_template Edit your template to fix the default values. Create a new PESTO project using your own template : pesto init -t /path/to/my_pesto_template /path/to/your/workspace/xxx-service","title":"init"},{"location":"pesto_init.html#pesto-init-create-a-new-project","text":"The first step to package your processing library is to create a new project. We encourage the following naming convention : given xxx a short name for your processing : then call xxx-lib : the processing library, and call xxx-service : the PESTO packaging project.","title":"pesto init : create a new project"},{"location":"pesto_init.html#basic-usage","text":"In a terminal : pesto init /path/to/your/workspace This will create a new project named \"/path/to/your/workspace/xxx-service\". The project is ready and setup for a simple processing, but you should at least edit some configuration files to tune PESTO to your needs. define the API pesto/api/input_schema.json : json schema describing the input payloads, pesto/api/output_schema.json : json schema describing the output payloads, implement your process algorithm/process.py : implementation of the processing, package pesto/api/description.json : informative description of the algorithm including deployment requirements, pesto/build/requirements.json : required files and library (your model is defined here) to build the web service.","title":"Basic usage"},{"location":"pesto_init.html#advanced-usage","text":"If you have many project sharing some information (your company, email, requirements ...) you can create a specific template. Copy the default template to a new place for your own template : pip show pesto-cli | grep Location | awk '{print $NF}' > /tmp/pesto_site_packages.txt cp -r `cat /tmp/pesto_site_packages.txt`/pesto_cli/resources/pesto-template /path/to/my_pesto_template Edit your template to fix the default values. Create a new PESTO project using your own template : pesto init -t /path/to/my_pesto_template /path/to/your/workspace/xxx-service","title":"Advanced usage"},{"location":"pesto_test.html","text":"pesto test : test your packaged docker image PESTO integrate a minimal yet powerful testing framework. Basic usage You must first configure your pesto/tests/resources folder by creating a new my_test_data folder inside. The pesto/tests/resources/my_test_data must contains : input : list of files to create the processing payload, output : list of files to create the expected dictionary response from the processing. Then, make sure that pesto/tests/test_service.py contains a test pointing to your my_test_data folder. Finally run the following command : pesto test /path/to/service-xxx Alternative: Use pytest to run the tests : pytest /path/to/service-xxx/pesto/tests Example: Each file in the input and output folder is interpreted following the following rules : the extension must be one of 'string', 'int', 'float', 'json' or some image extension (jpg, bmp, tif are supported).","title":"test"},{"location":"pesto_test.html#pesto-test-test-your-packaged-docker-image","text":"PESTO integrate a minimal yet powerful testing framework.","title":"pesto test : test your packaged docker image"},{"location":"pesto_test.html#basic-usage","text":"You must first configure your pesto/tests/resources folder by creating a new my_test_data folder inside. The pesto/tests/resources/my_test_data must contains : input : list of files to create the processing payload, output : list of files to create the expected dictionary response from the processing. Then, make sure that pesto/tests/test_service.py contains a test pointing to your my_test_data folder. Finally run the following command : pesto test /path/to/service-xxx Alternative: Use pytest to run the tests : pytest /path/to/service-xxx/pesto/tests Example: Each file in the input and output folder is interpreted following the following rules : the extension must be one of 'string', 'int', 'float', 'json' or some image extension (jpg, bmp, tif are supported).","title":"Basic usage"},{"location":"tutorial_pytorch.html","text":"Deploying PyTorch in Python via a REST API with PESTO Abstract This tutorial is inspired from the official Deploying PyTorch in Python via a REST API with Flask tutorial. In this walkthrough, you will be guided in using PESTO for packaging your Deep Learning Model such that it is ready for deployment in production. You will be able to send processing requests to your newly created web service embedding your own inference model. This model is a Resnet 50 CNN trained on ImageNet, and takes as input an image and returns one of the 1000 imagenet classes. During this tutorial you will learn to Package a model using PESTO Define the input and output API of you web service Generate the web service Docker image Deploy your webservice Send requests & get responses from the service What is ProcESsing facTOry ? PESTO is a packaging tool inspired from pip, maven and similar dependencies managers. It contains shell tools to generate all the boiler plate to build an OpenAPI processing web service compliant with the Geoprocessing-API . PESTO is designed to ease the process of packaging a Python algorithm as a processing web service into a docker image. The deployment of a web service becomes now as easy as filling few configuration files. PESTO is composed of the following components: pesto-cli : the command line interface used to create a PESTO project and package processing algorithms. pesto-ws : the processing web services, as docker images, created by PESTO to expose your algorithm. pesto-project : the project workspace created by pesto init to configure the packaging process of your algorithm. In the following tutorial we will generate a pesto-project and build a Docker Image containing a webservice that allows us to send requests to the model that we want to deploy into production. What are the differences between PESTO and Flask ? Flask is a Web Application framework requiring you to write the entire codebase and defining your API from scratch. PESTO is a framework allowing you to just describe the input / output of your model, write your custom code and bring dependencies. It will build the webservice for you (before, PESTO used flask but now it uses sanic ). Step by Step Walktrough First, ensure you have PESTO installed in a python 3.6+ environment. Typically you can use Miniconda as a virtual env. Then you should have docker community edition installed and configured in your machine. Refer to the docker documentation for more details. To install PESTO: git clone {} && cd processing-factory then make install (or cd pesto-cli && pip install . ) Next, we will initialize our PESTO template for our deployment, pesto init {location of the root folder where you will create the tutorial You will be prompted for several information to fill the default template. Here's an example of the display --------------------------------------------------------------------------------------------------------------------------- ____ _____ ____ _____ ___ ____ _ __ _ | _ \\| ____/ ___|_ _/ _ \\ _ | _ \\ _ __ ___ ___ ___ ___ ___(_)_ __ __ _ / _| __ _ ___| |_ ___ _ __ _ _ | |_) | _| \\___ \\ | || | | | (_) | |_) | '__/ _ \\ / __/ _ \\/ __/ __| | '_ \\ / _` | | |_ / _` |/ __| __/ _ \\| '__| | | | | __/| |___ ___) || || |_| | _ | __/| | | (_) | (_| __/\\__ \\__ \\ | | | | (_| | | _| (_| | (__| || (_) | | | |_| | |_| |_____|____/ |_| \\___/ (_) |_| |_| \\___/ \\___\\___||___/___/_|_| |_|\\__, | |_| \\__,_|\\___|\\__\\___/|_| \\__, | |___/ |___/ ----- ProcESsing facTOry : 1.0.0-rc1 ------------------------------------------------------------------------------------- cookiecutter /home/fchouteau/repositories/processing-factory/pesto-cli/pesto/cli/resources/template --output-dir . Please fill necessary information to initialize your template maintainer_fullname [TESUI]: Computer Vision maintainer_email [computervision@airbus.com]: project_name [algo-service]: pytorch-deployment-tutorial project_sname [pytorch-deployment-tutorial]: project_short_description [PESTO Template contains all the boilerplate you need to create a processing-factory project]: My first deployment with PESTO project_version [1.0.0.dev0]: 1.0.0 You will generate the default template in a folder pytorch-deployment-tutorial with the following file structure: pytorch-deployment-tutorial/ \u251c\u2500\u2500 algorithm \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 process.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 pesto \u2502 \u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 build \u2502 \u2514\u2500\u2500 tests \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 setup.py You can recognize a python package with a package named algorithm , a module algorithm.process . The main processing will be defined here (in Python and using your custom librairies if you want to do so) The folder pesto includes the necessary resources to build the docker image containing the service: pesto/api will specify the input/output of our process in terms of RESTful API pesto/build will specify resources, docker images, etc ... so that PESTO can build the service with the correct dependencies pesto/test will contains resources to test & debug your service once built as well as helper scripts to use RestFul API Presentation PESTO takes care of the API definition & endpoints through the Geoprocessing API specification . From a user point of view, several endpoints are defined: /api/v1/health where a GET request will sendback information /api/v1/describe for a GET request to get information about the processing encapsulated /api/v1/process where we will send the payload that we want to process via a POST request The next steps of this tutorial will focus on how we configure these endpoints so that the input/output API of our deployed service will allow our service to fulfill its expected role. To learn more about RESTful API, check this tutorial Your Custom Processing code Tip Due to the way we will load pesto-defined files in our process.py, (as well as custom dependencies unpacked at specific locations), it is hard to locally test the custom processing code without rewriting part of it to work locally. This is a known difficulty in development, we recommend to wrap your codebase under a custom library or package and to write as little code as possible (loading models, calling the prediction library then formatting the result properly) under process.py First, we will specify our inference pipeline. Our objective is to use a pretrained Convolutional Neural Network (A Resnet50) from torchvision to predict classes for image that will be fed to it. The model was trained on ImageNet so it should return one amongst 1000 classes when presented with an image. We will load our model, using the included checkpoints loading function of torchvision, as well as a json file containing the conversion between class indexes and class names (which is stored in /etc/pesto/config.json , more on that later). import json import torchvision.models with open(os.path.join(\"/etc/pesto/\", \"config.json\"), 'r') as f: IMAGENET_CLASS_INDEX = json.load(f) # Make sure to pass `pretrained` as `True` to use the pretrained weights: MODEL = models.alexnet(pretrained=True) # Since we are using our model only for inference, switch to `eval` mode: MODEL.eval() Resnet model requires the image to be of 3 channel RGB image of size 224 x 224. We will preprocess the image with Imagenet values as well. Info Should you require more information , please refer to the original tutorial as well as the pytorch documentation from PIL import Image import torchvision.transforms # Preprocessing function def transform_image(image: Image): my_transforms = torchvision.transforms.Compose([ torchvision.transforms.Resize(255), torchvision.transforms.CenterCrop(224), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) return my_transforms(image).unsqueeze(0) Now, getting predictions from this model is simple: import time def predict(image: np.ndarray): \"\"\" The core algorithm is implemented here. \"\"\" pil_image = Image.fromarray(image) tensor = transform_image(image=pil_image) outputs = MODEL.forward(tensor) _, y_hat = outputs.max(1) predicted_idx = str(y_hat.item()) class_id, class_name = IMAGENET_CLASS_INDEX[predicted_idx] result = {'category': class_name, 'time'} return result Now we will need to put these functions so that PESTO can properly call them. Look at algorithm/process.py This is the module that will be loaded by PESTO inside our server and which will be called during preprocessing. There is a Process class with on_start() and process() methods. The on_start() method will be called on the first processing request, it is usually useful to load resources etc. The process() function is called during call to /api/v1/process , when we want to actually process input data We want to integrate our previous code into this structure, so your algorithm/process.py file should look like this (replace the existing process.py file by this code, or write your own) Note We did not load the Model in the Process class so each method inside the Process class is static. import json import os import numpy as np import torch.cuda import torchvision.models import torchvision.transforms from PIL import Image # Device Agnostic Code if torch.cuda.is_available(): device = torch.device('cuda') cpu = torch.device('cpu') else: device = torch.device('cpu') cpu = torch.device('cpu') # Load Classes with open(os.path.join(\"/etc/pesto/\", \"config.json\"), \"r\") as f: IMAGENET_CLASS_INDEX = json.load(f) # Load Model # Make sure to pass `pretrained` as `True` to use the pretrained weights: MODEL = torchvision.models.resnet50(pretrained=True) if torch.cuda.is_available(): MODEL = MODEL.to(device) # Since we are using our model only for inference, switch to `eval` mode: MODEL.eval() class Process: @staticmethod def on_start() -> None: \"\"\" Process.on_start will be called at server start time. If you need to load heavy resources before processing data, this should be done here. \"\"\" image = (np.random.random((256, 256, 3)) * 255.0).astype(np.uint8) image = Image.fromarray(image) tensor = Process.transform_image(image).to(device) _ = MODEL.forward(tensor) print(\"DUMMY RUN OK\") # Preprocessing function @staticmethod def transform_image(image: Image): my_transforms = torchvision.transforms.Compose( [ torchvision.transforms.Resize(255), torchvision.transforms.CenterCrop(224), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225] ), ] ) return my_transforms(image).unsqueeze(0) # Main processing function @staticmethod def process(image: np.ndarray): \"\"\" The core algorithm is implemented here. \"\"\" # PESTO gives images as C,H,W so we will convert them back to H,W,C to convert them as PIL.Image image = image.transpose((1, 2, 0)) pil_image = Image.fromarray(image) # A tensor with a batch size of 1 (1, C, H, W) tensor = Process.transform_image(image=pil_image) if torch.cuda.is_available(): tensor = tensor.to(device) # Forward outputs = MODEL.forward(tensor) if torch.cuda.is_available(): outputs = outputs.to(cpu) # Postprocess _, y_hat = outputs.max(1) predicted_idx = str(y_hat.item()) class_id, class_name = IMAGENET_CLASS_INDEX[predicted_idx] # Always return a dictionary, as in RestFUL API, return type is a POST request result = {\"category\": class_name} return result About Images Format PESTO decodes input request in a specific way, which means that for images they are provided to the algorithm in Channel,Height,Width format, contrarily to the usual Height,Width,Channel format. This means that a transposition is required to wrap them up in PIL format for example. The easiest way to do so is to call image = image.transpose((1, 2, 0)) Configuring the Processing API for our service Now that we have implemented our processing, we will configure the web service API to map the RestAPI with the processing API. Let's have a look at the pesto/api folder : pesto/api/ \u251c\u2500\u2500 config.json \u251c\u2500\u2500 config_schema.json \u251c\u2500\u2500 description.json \u251c\u2500\u2500 description.stateful.json \u251c\u2500\u2500 input_schema.json \u251c\u2500\u2500 output_schema.json \u2514\u2500\u2500 version.json config.json is a static file which will be available config_schema.json is a json schema file that specifies what config.json should look like description.json is a json file that contains information about our processing input_schema.json is the specification of the input payload that is necessary to run Process.process() . It will be used to specify what should be sent to the webserver output_schema.json is the specification of the output response of the processing description.stateful.json is an alternative description that will be used with a different profile. The later parts of the tutorial will address this point specifically. For more information on jsonschema please refer to the official documentation config.json In config.json you can put any information that will be used later to configure your algorithm. This can be useful when used in conjunction with profiles, should you have different configuration for different profiles. In our use case, we will simply put all the imagenet classes in this file, so that they are readily acessible by the webservice. Download this file and copy it as config.json Imagenet classes can then be loaded in process.py as follows: # Load Classes with open(os.path.join(\"/etc/pesto/\", \"config.json\"), \"r\") as f: IMAGENET_CLASS_INDEX = json.load(f) Now we have to define the jsonschema ( config_schema.json ) that validates it. Here it is: { \"$schema\": \"http://json-schema.org/draft-06/schema#\", \"title\": \"tile-object-detection-config\", \"description\": \"Geo Process API config Schema for Deployment Tutorial\", \"type\": \"object\", \"patternProperties\": { \"^.*$\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } } } } Tip You can use the following code snippet to check for json schema validity in python import json import jsonschema with open('./config.json', 'r') as f: config = json.load(f) with open('./config_schema.json', 'r') as f: schema = json.load(f) jsonschema.validate(config, schema) description.json The description.json file contains information that describe your processing. Feel free to fill as much information as possible. Note that those information are INFORMATIVE only and not used anywhere except for the stateful key which has to set. For now, leave it to false , we will come back on it later. Here is an example of a description.json file that you can copy: { \"title\": \"pytorch-deployment-tutorial\", \"name\": \"pytorch-deployment-tutorial\", \"version\": \"1.0.0.dev0\", \"description\": \"My first deployment with PESTO\", \"family\": \"classification\", \"template\": \"image-classification\", \"keywords\": [ \"classification\", \"resnet\", \"imagenet\" ], \"resources\": { \"cpu\": 4, \"gpu\": 0, \"ram\": 8 }, \"asynchronous\": false, \"organization\": \"Computer Vision\", \"email\": \"computervision@airbus.com\", \"licence\": \"Property of Computer Vision, all rights reserved\" } input_schema.json Now comes the part where we will specify what types of request should an user send to the process. We will define a json schema that will automatically be parsed and serves as a validation checker to process incoming requests. We know that our process takes as input an image called image , so our input_schema.json file will look like this. { \"image\": { \"$ref\": \"#/definitions/Image\", \"description\": \"Image related to any ImageNet class\" }, \"required\": [ \"image\" ] } The \"$ref\": \"#/definitions/Image\" is a pointer to a custom PESTO type for json schema which is actually \"Image\": { \"$schema\": \"http://json-schema.org/draft-06/schema#\", \"description\": \"Image to process : it can be an url or the raw bytes encoded in base64\", \"type\": \"string\" }, Default PESTO types can be found in the source code : processing-factory/pesto-cli/pesto/cli/resources/schema/definitions.json By making an explicit reference to the custom processing factory type Image , PESTO will decode the image into a numpy array, and will accept either an uri or a raw bytes array encoded as base64. In the default template you can find several examples of input and output types, should you need to pass additional information to the process. output_schema.json The output_schema is the equivalent of the input_schema to parse results from the process (and specify the API response) In our case, the process will return the class name predicted by the image (a string), under the key category { \"category\": { \"type\": \"string\" } } In the default template you can find several examples of input and output types should you need to pass additional information to the process. Defining our packaging & dependencies Now that we have specified our API, let's take on the building part of PESTO. The principle of PESTO is that a Docker image with a webservice containing your processing will be constructed when we call pesto build . The next steps will be configuring PESTO to build a correct docker. Python dependencies for the project & requirements.txt The project we created is a python package and will be installed as such inside our docker. It is possible to specify the python requirements directly in requirements.txt as it will be parsed when doing pip install {our project} The alternative method would be to provide a docker base image with everything already installed, but this is a more advanced usage. For now, the requirements.txt file at the root of our project should look like this: numpy Pillow torch>=1.5.0 torchvision>=0.6.0 Now let's look at the pesto/build folder build/ \u251c\u2500\u2500 build.json \u251c\u2500\u2500 requirements.cpu.json \u2514\u2500\u2500 requirements.json Service Name & build.json The build.json contains automatically generated information that will be used by PESTO later. You should not have to modify it except if you want to change the version { \"name\": \"pytorch-deployment-tutorial\", \"version\": \"1.0.0.dev0\", \"algorithm_path\": null, \"workspace\": null } The docker image you will generate during build will be tagged name:version . File Dependencies & requirements.json There are two requirements.json files automatically generated. requirements.gpu.json defines a profile for GPU support and we will see later how to configure it. The requirements.json file default as such { \"environments\": {}, \"requirements\": {}, \"dockerBaseImage\": \"python:3.6-stretch\" } dockerBaseImage is a pointer towards a docker image that will be used to build the webservice. PESTO will inherit from this base image to install itself as well as the process and its dependencies. For now, python:3.6-stretch is a good starting point as it contains the necessary resources installed. You can pass a custom docker image to this step, provided your docker client can access it. environments is used to set environment variables. We will set the $TORCH_HOME environment variable to ensure we know its location. The $TORCH_HOME variable is used by torchvision to download weights in specific locations, check the torch Hub documentation for more details \"environments\": { \"TORCH_HOME\": \"/opt/torch/\" } requirements is helpful to add static resources such as model weights, configs, as well as custom python package. For now, requirements support two types of resources: static resources inside .tar.gz archives that will be uncompressed in environment python wheel .whl files that can be pip installed In order to try ourselves this mechanism, we will download the weights for our model. Torchvision models automatically do this by default when the models are called if the weights are not in $TORCH_HOME , but in our case we will put the weights ourselves so that no download step is done during runtime First, download this file wget https://download.pytorch.org/models/resnet50-19c8e357.pth Then put it into a .tar.gz archive accessible via either a uri ( file:// ), or an url ( gs:// and http:// are supported for now). Note that if you would deploy a model into production we recommend uploading resources to a server or committing them alongside your project so that everything is 100% reproducible tar -zcvf checkpoint.tar.gz resnet50-19c8e357.pth Now, note the uri of this checkpoint.tar.gz . We want to uncompress this file in /opt/torch/checkpoints/ in our docker. So your requirements file will look like: { \"environments\": { \"TORCH_HOME\": \"/opt/torch/\" }, \"requirements\": { \"checkpoints\": { \"from\": \"file://{PATH_TO_ARCHIVE}/checkpoint.tar.gz\", \"to\": \"/opt/torch/checkpoints/\" } }, \"dockerBaseImage\": \"python:3.6-stretch\" } Building the Service Now we have everything we need to build our service. The building part is simple: pesto build {root of your project}/pytorch-deployment-tutorial There will be a lot of logging informing you about what is happening. PESTO is using /home/$USER/.pesto/{process name}/{process version} to store resources needed to build the docker containing the service. Example in our case: pytorch-deployment-tutorial/ \u2514\u2500\u2500 1.0.0.dev0 \u251c\u2500\u2500 checkpoints \u2502 \u2514\u2500\u2500 resnet50-19c8e357.pth \u251c\u2500\u2500 dist \u2502 \u2514\u2500\u2500 pesto_cli-1.0.0rc1-py3-none-any.whl \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 pesto \u2502 \u2514\u2500\u2500 api_geo_process_v1.0.yaml \u251c\u2500\u2500 pytorch-deployment-tutorial \u2502 \u251c\u2500\u2500 algorithm \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 process.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 Makefile \u2502 \u251c\u2500\u2500 MANIFEST.in \u2502 \u251c\u2500\u2500 pesto \u2502 \u2502 \u251c\u2500\u2500 api \u2502 \u2502 \u2502 \u251c\u2500\u2500 config.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 config_schema.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 description.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 input_schema.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 output_schema.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 service.json \u2502 \u2502 \u2502 \u2514\u2500\u2500 version.json \u2502 \u2502 \u251c\u2500\u2500 build \u2502 \u2502 \u2502 \u251c\u2500\u2500 build.json \u2502 \u2502 \u2502 \u2514\u2500\u2500 requirements.json \u2502 \u2502 \u2514\u2500\u2500 tests \u2502 \u2502 \u251c\u2500\u2500 (...) \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 requirements.txt \u2502 \u2514\u2500\u2500 setup.py \u2514\u2500\u2500 requirements \u2514\u2500\u2500 checkpoint.tar.gz If docker build fails you can debug your service directly in this folder. If the build succeeds you should be able to see your image docker image : REPOSITORY TAG IMAGE ID CREATED SIZE pytorch-deployment-tutorial 1.0.0.dev0 08342775a658 4 minutes ago 3.48GB Testing and Usage Now we want to test if everything goes well, which means: Launching the docker container and checking that it responds to http requests Checking that the process we just deployed is working correctly Fortunately, PESTO features a test/usage framework which is the purpose of the pesto/test folder Booting up the container & first http requests First, we can verify that we are able to start the container and send very basic requests to it; Run docker run --rm -p 4000:8080 pytorch-deployment-tutorial:1.0.0.dev0 (check the docker documentation should you need help about the various arguments) This should start the container so that it can be accessed from http://localhost:4000. In your browser (or using CURL ) you can send basic GET requests to your container, such as http://localhost:4000/api/v1/health ( CURL -X GET http://localhost:4000/api/v1/health ) with should answer \"OK\" http://localhost:4000/api/v1/describe ( CURL -X GET http://localhost:4000/api/v1/describe ) which should return a json file It is recommended that you save said json file, it will be used later on CURL -X GET http://localhost:4000/api/v1/describe > descriptio.json Now the question is: How can I send a properly formated processing request with the payload (my image) that I want to send ? Tip If you know all about base64 encoding or sending URI with POST requests, feel free to skip this part. For the next parts you can safely stop your running container Defining Test Resources Let's take a look at the pesto/test directory tests \u251c\u2500\u2500 README.md \u251c\u2500\u2500 resources/ \u2502 \u251c\u2500\u2500 expected_describe.json \u2502 \u251c\u2500\u2500 test_1/ \u2502 \u2514\u2500\u2500 test_2/ The resources folder will be used by the PESTO Test API and be converted to processing requests that will be sent to /api/v1/process with the right format. The response will then be compared to the expected response, and act as unit tests. Note In the later part of this tutorial we will showcase three different ways of generating processing payloads and getting responses / comparing to expected responses. Each method can be used in different context, using different abstraction levels. The first file of interest is the expected_describe.json . This file will be compared to the http://localhost:4000/api/v1/describe json document returned by the API. This description file can be used to parse the information about the API (input / output schema, description etc...) You will learn in time how to manually create an expected_describe.json from the pesto/api folder, for now it is best to copy the describe.json file that we generated earlier and to put it as expected_describe.json . You can compare this file to the default expected_describe.json and notice how the differences translate themselves to the default processing Now, there are several folders named test_* . The purpose of these test folders is that the input payload files are deposited in input and the expected response is in output Let's take a look at the test folder: test_1 \u251c\u2500\u2500 input \u2502 \u251c\u2500\u2500 dict_parameter.json \u2502 \u251c\u2500\u2500 image.png \u2502 \u251c\u2500\u2500 integer_parameter.int \u2502 \u251c\u2500\u2500 number_parameter.float \u2502 \u251c\u2500\u2500 object_parameter.json \u2502 \u2514\u2500\u2500 string_parameter.string \u2514\u2500\u2500 output \u251c\u2500\u2500 areas \u2502 \u251c\u2500\u2500 0.json \u2502 \u2514\u2500\u2500 1.json \u251c\u2500\u2500 dict_output.json \u251c\u2500\u2500 geojson.json \u251c\u2500\u2500 image_list \u2502 \u251c\u2500\u2500 0.png \u2502 \u2514\u2500\u2500 1.png \u251c\u2500\u2500 image.png \u251c\u2500\u2500 integer_output.integer \u251c\u2500\u2500 number_output.float \u2514\u2500\u2500 string_output.string You can see that both input and output have files with extension corresponding to input types. The filenames are matched with the json payload keys. Now, we are going to write two tests with those two images as input: We know that the input key is image and the output key is category the model should predict {\"category\": \"Egyptian_cat\"} We know that the input key is image and the output key is category the model should predict {\"category\": \"mortar\"} So, your folder structure should now look like: tests/ \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 expected_describe.json \u2502 \u251c\u2500\u2500 test_1 (cat) \u2502 \u2502 \u251c\u2500\u2500 input \u2502 \u2502 \u2502 \u2514\u2500\u2500 image.png <- copy the cat image here \u2502 \u2502 \u2514\u2500\u2500 output \u2502 \u2502 \u2514\u2500\u2500 category.string <- this should be Egypcatian_cat \u2502 \u2514\u2500\u2500 test_2 (pesto) \u2502 \u251c\u2500\u2500 input \u2502 \u2502 \u2514\u2500\u2500 image.jpg <- copy the pesto image here \u2502 \u2514\u2500\u2500 output \u2502 \u2514\u2500\u2500 category.string <- this should be mortar Using pesto test command The first way of testing your service is to call pesto test utility the same way you called pesto build . In order, this command will: Run the docker container (the same way we did previously) Send requests to api/v1/describe and compare with the expected_describe.json Send process payloads to api/v1/process and compare them to the desired outputs In your project root, run pesto test . and check what happens. The logs should show different steps being processed. You can check the responses and differences between dictionnaries in the .pesto workspace: /home/$USER/.pesto/tests/pytorch-deployment-tutorial/1.0.0.dev0 You will find there the results / responses of all the requests, including describes and processings requests. This is a useful folder to debug potential differences. Should everything goes well, the results.json file should look like this { \"describe\": { \"NoDifference\": true }, \"test_1\": { \"NoDifference\": true }, \"test_2\": { \"NoDifference\": true } } Note pesto test is designed not to fail if the requests pass; Instead it will simply compare dictionaries and display / save the differences as well as the responses, so that the user can go look at what happened and check if this is correct. pesto test should be used for debug purposed and not for unit test purposes. We will see later how we can use the PESTO test API with pytest to actually run unit tests Bonus: Using Pytest & unit testing Once you're sure and have debugged properly you can write or edit unit tests in project-root/tests/ (check the autogenerated file tests/test_service.py ) and run it with pytest tests on your root project This can be used to ensure non regression on further edits or if you want to do test driver development Bonus: Using PESTO Python API to run tests & send requests to model Should you want to use in a non-scalable way or further test your services, you can have a look at the {project-root}/scripts/example_api_usage.py file that exposes the low level python API that is used in pesto test The ServiceManager class is the class used as a proxy for the python Docker API, and is used to pull / run /attach / stop the containers The PayloadGenerator class is used to translate files to actual json payload for the REST API The EndpointManager manages the various endpoints of the processes, and act as a front to post/get requests The ServiceTester is used to validate payloads & responses against their expected values Note This API is a simple example of how to use services packaged with pesto in python scripts. We encourage you to copy/paste and modify the classes should you feel the need for specific use cases, but both this and pesto test is not designed for robustness and scalability We consider the target of pesto test capabilities to be the data scientist, integration testing & scalability should be done at production level Adding a GPU profile In order to create an image with GPU support, we can complete the proposed profile gpu . The file requirements.gpu.json can be updated as follows : { \"environments\": {}, \"requirements\": {}, \"dockerBaseImage\": \"pytorch/pytorch:1.5-cuda10.1-cudnn7-runtime\" } You can now build your GPU enabled microservice : pesto build {root of your project}/pytorch-deployment-tutorial -p gpu PESTO Profiles in order to accomodate for different hardware targets or slight variations of the same process to deploy, PESTO has a built-in capabilities called profiles Basically, PESTO profiles is a list of ordered strings ( gpu stateless ) whose .json files in build/api folders sequentially update the base file. To use them, simply add the list of profiles to your PESTO commands: pesto build {project-root} -p p1 p2 or pesto test {project-root} -p p1 p2 The profiles json files are named {original_file}.{profile}.json . For example, for a description.json , then the corresponding description.json for the profile gpu would be description.gpu.json . Profile jsons can be partially complete as they only update the base values if the files are present. Example: description.json : {\"key\":\"value\", \"key2\":\"value2\"} description.p1.json : {\"key\":\"value3\"} Then calling pesto build . -p p1 will generate a description.json : {\"key\":\"value3\", \"key2\":\"value2\"} and take all the other files without additionnal profiles. Warning Due to the sequential nature of dictionary updates, the profiles are order dependent If you have a computer with nvidia drivers & a gpu you can try to run pesto build . -p gpu and pesto test . -p gpu --nvidia which should do the same as above but with gpu support (and actually run the process on gpu) Stateful & Stateless services PESTO supports building stateless services as well as stateful services . With stateless services, it is expected that the processing replies directly to the processing request with the response. These services should have no internal state and should always return the same result when presented with the same payload Stateful services can have internal states, and store the processing results to be later queried. The main difference is that sending a processing request to api/v1/process to a stateful service will not return the result but a jobID . The job state can be quiered at GET api/v1/jobs/{jobID}/status and results can be queried at GET api/v1/jobs/{jobID}/results when the job is done. The response of the latter request will be a json matching the output schema with URI to individual content (that should individually be queried using GET requests ) Try building your previous service with pesto build . -p stateful and starting it as previously, docker run --rm -p 4000:8080 pytorch-deployment-tutorial:1.0.0.dev0-stateful Then, run the API usage script ( python scripts/example_api_usage ) while having modified the image name to stateful. This script should send several requests (like pesto test), but the advantage is that it doesn't kill the service afterwards, so it is possible to look at what happened: Try doing a get request on /api/v1/jobs/ you should see a list of jobs Grab a job id then do a GET request on /api/v1/jobs/{jobID}/status . It should be \"DONE\" Then do a GET request on /api/v1/jobs/{jobID}/results to get results You should get something like { \"category\": \"http://localhost:4000/api/v1/jobs/1080019533/results/category\" } A GET request on the aforementioned URL should return Egyptian_cat or mortar Next Steps You should version your PESTO project using git so that it is reproducible The rest of the documentation should be more accessible now that you have completed this tutorial Feel free to send us feedback and ask any question on github There are some advanced usage & tips in the pesto cookbook . If you find an use case that is not documented, feel free to submit a PR on github to update the documentation","title":"Deploying a pytorch model"},{"location":"tutorial_pytorch.html#deploying-pytorch-in-python-via-a-rest-api-with-pesto","text":"Abstract This tutorial is inspired from the official Deploying PyTorch in Python via a REST API with Flask tutorial. In this walkthrough, you will be guided in using PESTO for packaging your Deep Learning Model such that it is ready for deployment in production. You will be able to send processing requests to your newly created web service embedding your own inference model. This model is a Resnet 50 CNN trained on ImageNet, and takes as input an image and returns one of the 1000 imagenet classes. During this tutorial you will learn to Package a model using PESTO Define the input and output API of you web service Generate the web service Docker image Deploy your webservice Send requests & get responses from the service","title":"Deploying PyTorch in Python via a REST API with PESTO"},{"location":"tutorial_pytorch.html#what-is-processing-factory","text":"PESTO is a packaging tool inspired from pip, maven and similar dependencies managers. It contains shell tools to generate all the boiler plate to build an OpenAPI processing web service compliant with the Geoprocessing-API . PESTO is designed to ease the process of packaging a Python algorithm as a processing web service into a docker image. The deployment of a web service becomes now as easy as filling few configuration files. PESTO is composed of the following components: pesto-cli : the command line interface used to create a PESTO project and package processing algorithms. pesto-ws : the processing web services, as docker images, created by PESTO to expose your algorithm. pesto-project : the project workspace created by pesto init to configure the packaging process of your algorithm. In the following tutorial we will generate a pesto-project and build a Docker Image containing a webservice that allows us to send requests to the model that we want to deploy into production.","title":"What is ProcESsing facTOry ?"},{"location":"tutorial_pytorch.html#what-are-the-differences-between-pesto-and-flask","text":"Flask is a Web Application framework requiring you to write the entire codebase and defining your API from scratch. PESTO is a framework allowing you to just describe the input / output of your model, write your custom code and bring dependencies. It will build the webservice for you (before, PESTO used flask but now it uses sanic ).","title":"What are the differences between PESTO and Flask ?"},{"location":"tutorial_pytorch.html#step-by-step-walktrough","text":"First, ensure you have PESTO installed in a python 3.6+ environment. Typically you can use Miniconda as a virtual env. Then you should have docker community edition installed and configured in your machine. Refer to the docker documentation for more details. To install PESTO: git clone {} && cd processing-factory then make install (or cd pesto-cli && pip install . ) Next, we will initialize our PESTO template for our deployment, pesto init {location of the root folder where you will create the tutorial You will be prompted for several information to fill the default template. Here's an example of the display --------------------------------------------------------------------------------------------------------------------------- ____ _____ ____ _____ ___ ____ _ __ _ | _ \\| ____/ ___|_ _/ _ \\ _ | _ \\ _ __ ___ ___ ___ ___ ___(_)_ __ __ _ / _| __ _ ___| |_ ___ _ __ _ _ | |_) | _| \\___ \\ | || | | | (_) | |_) | '__/ _ \\ / __/ _ \\/ __/ __| | '_ \\ / _` | | |_ / _` |/ __| __/ _ \\| '__| | | | | __/| |___ ___) || || |_| | _ | __/| | | (_) | (_| __/\\__ \\__ \\ | | | | (_| | | _| (_| | (__| || (_) | | | |_| | |_| |_____|____/ |_| \\___/ (_) |_| |_| \\___/ \\___\\___||___/___/_|_| |_|\\__, | |_| \\__,_|\\___|\\__\\___/|_| \\__, | |___/ |___/ ----- ProcESsing facTOry : 1.0.0-rc1 ------------------------------------------------------------------------------------- cookiecutter /home/fchouteau/repositories/processing-factory/pesto-cli/pesto/cli/resources/template --output-dir . Please fill necessary information to initialize your template maintainer_fullname [TESUI]: Computer Vision maintainer_email [computervision@airbus.com]: project_name [algo-service]: pytorch-deployment-tutorial project_sname [pytorch-deployment-tutorial]: project_short_description [PESTO Template contains all the boilerplate you need to create a processing-factory project]: My first deployment with PESTO project_version [1.0.0.dev0]: 1.0.0 You will generate the default template in a folder pytorch-deployment-tutorial with the following file structure: pytorch-deployment-tutorial/ \u251c\u2500\u2500 algorithm \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 process.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 pesto \u2502 \u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 build \u2502 \u2514\u2500\u2500 tests \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 setup.py You can recognize a python package with a package named algorithm , a module algorithm.process . The main processing will be defined here (in Python and using your custom librairies if you want to do so) The folder pesto includes the necessary resources to build the docker image containing the service: pesto/api will specify the input/output of our process in terms of RESTful API pesto/build will specify resources, docker images, etc ... so that PESTO can build the service with the correct dependencies pesto/test will contains resources to test & debug your service once built as well as helper scripts to use","title":"Step by Step Walktrough"},{"location":"tutorial_pytorch.html#restful-api-presentation","text":"PESTO takes care of the API definition & endpoints through the Geoprocessing API specification . From a user point of view, several endpoints are defined: /api/v1/health where a GET request will sendback information /api/v1/describe for a GET request to get information about the processing encapsulated /api/v1/process where we will send the payload that we want to process via a POST request The next steps of this tutorial will focus on how we configure these endpoints so that the input/output API of our deployed service will allow our service to fulfill its expected role. To learn more about RESTful API, check this tutorial","title":"RestFul API Presentation"},{"location":"tutorial_pytorch.html#your-custom-processing-code","text":"Tip Due to the way we will load pesto-defined files in our process.py, (as well as custom dependencies unpacked at specific locations), it is hard to locally test the custom processing code without rewriting part of it to work locally. This is a known difficulty in development, we recommend to wrap your codebase under a custom library or package and to write as little code as possible (loading models, calling the prediction library then formatting the result properly) under process.py First, we will specify our inference pipeline. Our objective is to use a pretrained Convolutional Neural Network (A Resnet50) from torchvision to predict classes for image that will be fed to it. The model was trained on ImageNet so it should return one amongst 1000 classes when presented with an image. We will load our model, using the included checkpoints loading function of torchvision, as well as a json file containing the conversion between class indexes and class names (which is stored in /etc/pesto/config.json , more on that later). import json import torchvision.models with open(os.path.join(\"/etc/pesto/\", \"config.json\"), 'r') as f: IMAGENET_CLASS_INDEX = json.load(f) # Make sure to pass `pretrained` as `True` to use the pretrained weights: MODEL = models.alexnet(pretrained=True) # Since we are using our model only for inference, switch to `eval` mode: MODEL.eval() Resnet model requires the image to be of 3 channel RGB image of size 224 x 224. We will preprocess the image with Imagenet values as well. Info Should you require more information , please refer to the original tutorial as well as the pytorch documentation from PIL import Image import torchvision.transforms # Preprocessing function def transform_image(image: Image): my_transforms = torchvision.transforms.Compose([ torchvision.transforms.Resize(255), torchvision.transforms.CenterCrop(224), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) return my_transforms(image).unsqueeze(0) Now, getting predictions from this model is simple: import time def predict(image: np.ndarray): \"\"\" The core algorithm is implemented here. \"\"\" pil_image = Image.fromarray(image) tensor = transform_image(image=pil_image) outputs = MODEL.forward(tensor) _, y_hat = outputs.max(1) predicted_idx = str(y_hat.item()) class_id, class_name = IMAGENET_CLASS_INDEX[predicted_idx] result = {'category': class_name, 'time'} return result Now we will need to put these functions so that PESTO can properly call them. Look at algorithm/process.py This is the module that will be loaded by PESTO inside our server and which will be called during preprocessing. There is a Process class with on_start() and process() methods. The on_start() method will be called on the first processing request, it is usually useful to load resources etc. The process() function is called during call to /api/v1/process , when we want to actually process input data We want to integrate our previous code into this structure, so your algorithm/process.py file should look like this (replace the existing process.py file by this code, or write your own) Note We did not load the Model in the Process class so each method inside the Process class is static. import json import os import numpy as np import torch.cuda import torchvision.models import torchvision.transforms from PIL import Image # Device Agnostic Code if torch.cuda.is_available(): device = torch.device('cuda') cpu = torch.device('cpu') else: device = torch.device('cpu') cpu = torch.device('cpu') # Load Classes with open(os.path.join(\"/etc/pesto/\", \"config.json\"), \"r\") as f: IMAGENET_CLASS_INDEX = json.load(f) # Load Model # Make sure to pass `pretrained` as `True` to use the pretrained weights: MODEL = torchvision.models.resnet50(pretrained=True) if torch.cuda.is_available(): MODEL = MODEL.to(device) # Since we are using our model only for inference, switch to `eval` mode: MODEL.eval() class Process: @staticmethod def on_start() -> None: \"\"\" Process.on_start will be called at server start time. If you need to load heavy resources before processing data, this should be done here. \"\"\" image = (np.random.random((256, 256, 3)) * 255.0).astype(np.uint8) image = Image.fromarray(image) tensor = Process.transform_image(image).to(device) _ = MODEL.forward(tensor) print(\"DUMMY RUN OK\") # Preprocessing function @staticmethod def transform_image(image: Image): my_transforms = torchvision.transforms.Compose( [ torchvision.transforms.Resize(255), torchvision.transforms.CenterCrop(224), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225] ), ] ) return my_transforms(image).unsqueeze(0) # Main processing function @staticmethod def process(image: np.ndarray): \"\"\" The core algorithm is implemented here. \"\"\" # PESTO gives images as C,H,W so we will convert them back to H,W,C to convert them as PIL.Image image = image.transpose((1, 2, 0)) pil_image = Image.fromarray(image) # A tensor with a batch size of 1 (1, C, H, W) tensor = Process.transform_image(image=pil_image) if torch.cuda.is_available(): tensor = tensor.to(device) # Forward outputs = MODEL.forward(tensor) if torch.cuda.is_available(): outputs = outputs.to(cpu) # Postprocess _, y_hat = outputs.max(1) predicted_idx = str(y_hat.item()) class_id, class_name = IMAGENET_CLASS_INDEX[predicted_idx] # Always return a dictionary, as in RestFUL API, return type is a POST request result = {\"category\": class_name} return result About Images Format PESTO decodes input request in a specific way, which means that for images they are provided to the algorithm in Channel,Height,Width format, contrarily to the usual Height,Width,Channel format. This means that a transposition is required to wrap them up in PIL format for example. The easiest way to do so is to call image = image.transpose((1, 2, 0))","title":"Your Custom Processing code"},{"location":"tutorial_pytorch.html#configuring-the-processing-api-for-our-service","text":"Now that we have implemented our processing, we will configure the web service API to map the RestAPI with the processing API. Let's have a look at the pesto/api folder : pesto/api/ \u251c\u2500\u2500 config.json \u251c\u2500\u2500 config_schema.json \u251c\u2500\u2500 description.json \u251c\u2500\u2500 description.stateful.json \u251c\u2500\u2500 input_schema.json \u251c\u2500\u2500 output_schema.json \u2514\u2500\u2500 version.json config.json is a static file which will be available config_schema.json is a json schema file that specifies what config.json should look like description.json is a json file that contains information about our processing input_schema.json is the specification of the input payload that is necessary to run Process.process() . It will be used to specify what should be sent to the webserver output_schema.json is the specification of the output response of the processing description.stateful.json is an alternative description that will be used with a different profile. The later parts of the tutorial will address this point specifically. For more information on jsonschema please refer to the official documentation","title":"Configuring the Processing API for our service"},{"location":"tutorial_pytorch.html#configjson","text":"In config.json you can put any information that will be used later to configure your algorithm. This can be useful when used in conjunction with profiles, should you have different configuration for different profiles. In our use case, we will simply put all the imagenet classes in this file, so that they are readily acessible by the webservice. Download this file and copy it as config.json Imagenet classes can then be loaded in process.py as follows: # Load Classes with open(os.path.join(\"/etc/pesto/\", \"config.json\"), \"r\") as f: IMAGENET_CLASS_INDEX = json.load(f) Now we have to define the jsonschema ( config_schema.json ) that validates it. Here it is: { \"$schema\": \"http://json-schema.org/draft-06/schema#\", \"title\": \"tile-object-detection-config\", \"description\": \"Geo Process API config Schema for Deployment Tutorial\", \"type\": \"object\", \"patternProperties\": { \"^.*$\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } } } } Tip You can use the following code snippet to check for json schema validity in python import json import jsonschema with open('./config.json', 'r') as f: config = json.load(f) with open('./config_schema.json', 'r') as f: schema = json.load(f) jsonschema.validate(config, schema)","title":"config.json"},{"location":"tutorial_pytorch.html#descriptionjson","text":"The description.json file contains information that describe your processing. Feel free to fill as much information as possible. Note that those information are INFORMATIVE only and not used anywhere except for the stateful key which has to set. For now, leave it to false , we will come back on it later. Here is an example of a description.json file that you can copy: { \"title\": \"pytorch-deployment-tutorial\", \"name\": \"pytorch-deployment-tutorial\", \"version\": \"1.0.0.dev0\", \"description\": \"My first deployment with PESTO\", \"family\": \"classification\", \"template\": \"image-classification\", \"keywords\": [ \"classification\", \"resnet\", \"imagenet\" ], \"resources\": { \"cpu\": 4, \"gpu\": 0, \"ram\": 8 }, \"asynchronous\": false, \"organization\": \"Computer Vision\", \"email\": \"computervision@airbus.com\", \"licence\": \"Property of Computer Vision, all rights reserved\" }","title":"description.json"},{"location":"tutorial_pytorch.html#input_schemajson","text":"Now comes the part where we will specify what types of request should an user send to the process. We will define a json schema that will automatically be parsed and serves as a validation checker to process incoming requests. We know that our process takes as input an image called image , so our input_schema.json file will look like this. { \"image\": { \"$ref\": \"#/definitions/Image\", \"description\": \"Image related to any ImageNet class\" }, \"required\": [ \"image\" ] } The \"$ref\": \"#/definitions/Image\" is a pointer to a custom PESTO type for json schema which is actually \"Image\": { \"$schema\": \"http://json-schema.org/draft-06/schema#\", \"description\": \"Image to process : it can be an url or the raw bytes encoded in base64\", \"type\": \"string\" }, Default PESTO types can be found in the source code : processing-factory/pesto-cli/pesto/cli/resources/schema/definitions.json By making an explicit reference to the custom processing factory type Image , PESTO will decode the image into a numpy array, and will accept either an uri or a raw bytes array encoded as base64. In the default template you can find several examples of input and output types, should you need to pass additional information to the process.","title":"input_schema.json"},{"location":"tutorial_pytorch.html#output_schemajson","text":"The output_schema is the equivalent of the input_schema to parse results from the process (and specify the API response) In our case, the process will return the class name predicted by the image (a string), under the key category { \"category\": { \"type\": \"string\" } } In the default template you can find several examples of input and output types should you need to pass additional information to the process.","title":"output_schema.json"},{"location":"tutorial_pytorch.html#defining-our-packaging-dependencies","text":"Now that we have specified our API, let's take on the building part of PESTO. The principle of PESTO is that a Docker image with a webservice containing your processing will be constructed when we call pesto build . The next steps will be configuring PESTO to build a correct docker.","title":"Defining our packaging &amp; dependencies"},{"location":"tutorial_pytorch.html#python-dependencies-for-the-project-requirementstxt","text":"The project we created is a python package and will be installed as such inside our docker. It is possible to specify the python requirements directly in requirements.txt as it will be parsed when doing pip install {our project} The alternative method would be to provide a docker base image with everything already installed, but this is a more advanced usage. For now, the requirements.txt file at the root of our project should look like this: numpy Pillow torch>=1.5.0 torchvision>=0.6.0 Now let's look at the pesto/build folder build/ \u251c\u2500\u2500 build.json \u251c\u2500\u2500 requirements.cpu.json \u2514\u2500\u2500 requirements.json","title":"Python dependencies for the project &amp; requirements.txt"},{"location":"tutorial_pytorch.html#service-name-buildjson","text":"The build.json contains automatically generated information that will be used by PESTO later. You should not have to modify it except if you want to change the version { \"name\": \"pytorch-deployment-tutorial\", \"version\": \"1.0.0.dev0\", \"algorithm_path\": null, \"workspace\": null } The docker image you will generate during build will be tagged name:version .","title":"Service Name &amp; build.json"},{"location":"tutorial_pytorch.html#file-dependencies-requirementsjson","text":"There are two requirements.json files automatically generated. requirements.gpu.json defines a profile for GPU support and we will see later how to configure it. The requirements.json file default as such { \"environments\": {}, \"requirements\": {}, \"dockerBaseImage\": \"python:3.6-stretch\" } dockerBaseImage is a pointer towards a docker image that will be used to build the webservice. PESTO will inherit from this base image to install itself as well as the process and its dependencies. For now, python:3.6-stretch is a good starting point as it contains the necessary resources installed. You can pass a custom docker image to this step, provided your docker client can access it. environments is used to set environment variables. We will set the $TORCH_HOME environment variable to ensure we know its location. The $TORCH_HOME variable is used by torchvision to download weights in specific locations, check the torch Hub documentation for more details \"environments\": { \"TORCH_HOME\": \"/opt/torch/\" } requirements is helpful to add static resources such as model weights, configs, as well as custom python package. For now, requirements support two types of resources: static resources inside .tar.gz archives that will be uncompressed in environment python wheel .whl files that can be pip installed In order to try ourselves this mechanism, we will download the weights for our model. Torchvision models automatically do this by default when the models are called if the weights are not in $TORCH_HOME , but in our case we will put the weights ourselves so that no download step is done during runtime First, download this file wget https://download.pytorch.org/models/resnet50-19c8e357.pth Then put it into a .tar.gz archive accessible via either a uri ( file:// ), or an url ( gs:// and http:// are supported for now). Note that if you would deploy a model into production we recommend uploading resources to a server or committing them alongside your project so that everything is 100% reproducible tar -zcvf checkpoint.tar.gz resnet50-19c8e357.pth Now, note the uri of this checkpoint.tar.gz . We want to uncompress this file in /opt/torch/checkpoints/ in our docker. So your requirements file will look like: { \"environments\": { \"TORCH_HOME\": \"/opt/torch/\" }, \"requirements\": { \"checkpoints\": { \"from\": \"file://{PATH_TO_ARCHIVE}/checkpoint.tar.gz\", \"to\": \"/opt/torch/checkpoints/\" } }, \"dockerBaseImage\": \"python:3.6-stretch\" }","title":"File Dependencies &amp; requirements.json"},{"location":"tutorial_pytorch.html#building-the-service","text":"Now we have everything we need to build our service. The building part is simple: pesto build {root of your project}/pytorch-deployment-tutorial There will be a lot of logging informing you about what is happening. PESTO is using /home/$USER/.pesto/{process name}/{process version} to store resources needed to build the docker containing the service. Example in our case: pytorch-deployment-tutorial/ \u2514\u2500\u2500 1.0.0.dev0 \u251c\u2500\u2500 checkpoints \u2502 \u2514\u2500\u2500 resnet50-19c8e357.pth \u251c\u2500\u2500 dist \u2502 \u2514\u2500\u2500 pesto_cli-1.0.0rc1-py3-none-any.whl \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 pesto \u2502 \u2514\u2500\u2500 api_geo_process_v1.0.yaml \u251c\u2500\u2500 pytorch-deployment-tutorial \u2502 \u251c\u2500\u2500 algorithm \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 process.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 Makefile \u2502 \u251c\u2500\u2500 MANIFEST.in \u2502 \u251c\u2500\u2500 pesto \u2502 \u2502 \u251c\u2500\u2500 api \u2502 \u2502 \u2502 \u251c\u2500\u2500 config.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 config_schema.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 description.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 input_schema.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 output_schema.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 service.json \u2502 \u2502 \u2502 \u2514\u2500\u2500 version.json \u2502 \u2502 \u251c\u2500\u2500 build \u2502 \u2502 \u2502 \u251c\u2500\u2500 build.json \u2502 \u2502 \u2502 \u2514\u2500\u2500 requirements.json \u2502 \u2502 \u2514\u2500\u2500 tests \u2502 \u2502 \u251c\u2500\u2500 (...) \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 requirements.txt \u2502 \u2514\u2500\u2500 setup.py \u2514\u2500\u2500 requirements \u2514\u2500\u2500 checkpoint.tar.gz If docker build fails you can debug your service directly in this folder. If the build succeeds you should be able to see your image docker image : REPOSITORY TAG IMAGE ID CREATED SIZE pytorch-deployment-tutorial 1.0.0.dev0 08342775a658 4 minutes ago 3.48GB","title":"Building the Service"},{"location":"tutorial_pytorch.html#testing-and-usage","text":"Now we want to test if everything goes well, which means: Launching the docker container and checking that it responds to http requests Checking that the process we just deployed is working correctly Fortunately, PESTO features a test/usage framework which is the purpose of the pesto/test folder","title":"Testing and Usage"},{"location":"tutorial_pytorch.html#booting-up-the-container-first-http-requests","text":"First, we can verify that we are able to start the container and send very basic requests to it; Run docker run --rm -p 4000:8080 pytorch-deployment-tutorial:1.0.0.dev0 (check the docker documentation should you need help about the various arguments) This should start the container so that it can be accessed from http://localhost:4000. In your browser (or using CURL ) you can send basic GET requests to your container, such as http://localhost:4000/api/v1/health ( CURL -X GET http://localhost:4000/api/v1/health ) with should answer \"OK\" http://localhost:4000/api/v1/describe ( CURL -X GET http://localhost:4000/api/v1/describe ) which should return a json file It is recommended that you save said json file, it will be used later on CURL -X GET http://localhost:4000/api/v1/describe > descriptio.json Now the question is: How can I send a properly formated processing request with the payload (my image) that I want to send ? Tip If you know all about base64 encoding or sending URI with POST requests, feel free to skip this part. For the next parts you can safely stop your running container","title":"Booting up the container &amp; first http requests"},{"location":"tutorial_pytorch.html#defining-test-resources","text":"Let's take a look at the pesto/test directory tests \u251c\u2500\u2500 README.md \u251c\u2500\u2500 resources/ \u2502 \u251c\u2500\u2500 expected_describe.json \u2502 \u251c\u2500\u2500 test_1/ \u2502 \u2514\u2500\u2500 test_2/ The resources folder will be used by the PESTO Test API and be converted to processing requests that will be sent to /api/v1/process with the right format. The response will then be compared to the expected response, and act as unit tests. Note In the later part of this tutorial we will showcase three different ways of generating processing payloads and getting responses / comparing to expected responses. Each method can be used in different context, using different abstraction levels. The first file of interest is the expected_describe.json . This file will be compared to the http://localhost:4000/api/v1/describe json document returned by the API. This description file can be used to parse the information about the API (input / output schema, description etc...) You will learn in time how to manually create an expected_describe.json from the pesto/api folder, for now it is best to copy the describe.json file that we generated earlier and to put it as expected_describe.json . You can compare this file to the default expected_describe.json and notice how the differences translate themselves to the default processing Now, there are several folders named test_* . The purpose of these test folders is that the input payload files are deposited in input and the expected response is in output Let's take a look at the test folder: test_1 \u251c\u2500\u2500 input \u2502 \u251c\u2500\u2500 dict_parameter.json \u2502 \u251c\u2500\u2500 image.png \u2502 \u251c\u2500\u2500 integer_parameter.int \u2502 \u251c\u2500\u2500 number_parameter.float \u2502 \u251c\u2500\u2500 object_parameter.json \u2502 \u2514\u2500\u2500 string_parameter.string \u2514\u2500\u2500 output \u251c\u2500\u2500 areas \u2502 \u251c\u2500\u2500 0.json \u2502 \u2514\u2500\u2500 1.json \u251c\u2500\u2500 dict_output.json \u251c\u2500\u2500 geojson.json \u251c\u2500\u2500 image_list \u2502 \u251c\u2500\u2500 0.png \u2502 \u2514\u2500\u2500 1.png \u251c\u2500\u2500 image.png \u251c\u2500\u2500 integer_output.integer \u251c\u2500\u2500 number_output.float \u2514\u2500\u2500 string_output.string You can see that both input and output have files with extension corresponding to input types. The filenames are matched with the json payload keys. Now, we are going to write two tests with those two images as input: We know that the input key is image and the output key is category the model should predict {\"category\": \"Egyptian_cat\"} We know that the input key is image and the output key is category the model should predict {\"category\": \"mortar\"} So, your folder structure should now look like: tests/ \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 expected_describe.json \u2502 \u251c\u2500\u2500 test_1 (cat) \u2502 \u2502 \u251c\u2500\u2500 input \u2502 \u2502 \u2502 \u2514\u2500\u2500 image.png <- copy the cat image here \u2502 \u2502 \u2514\u2500\u2500 output \u2502 \u2502 \u2514\u2500\u2500 category.string <- this should be Egypcatian_cat \u2502 \u2514\u2500\u2500 test_2 (pesto) \u2502 \u251c\u2500\u2500 input \u2502 \u2502 \u2514\u2500\u2500 image.jpg <- copy the pesto image here \u2502 \u2514\u2500\u2500 output \u2502 \u2514\u2500\u2500 category.string <- this should be mortar","title":"Defining Test Resources"},{"location":"tutorial_pytorch.html#using-pesto-test-command","text":"The first way of testing your service is to call pesto test utility the same way you called pesto build . In order, this command will: Run the docker container (the same way we did previously) Send requests to api/v1/describe and compare with the expected_describe.json Send process payloads to api/v1/process and compare them to the desired outputs In your project root, run pesto test . and check what happens. The logs should show different steps being processed. You can check the responses and differences between dictionnaries in the .pesto workspace: /home/$USER/.pesto/tests/pytorch-deployment-tutorial/1.0.0.dev0 You will find there the results / responses of all the requests, including describes and processings requests. This is a useful folder to debug potential differences. Should everything goes well, the results.json file should look like this { \"describe\": { \"NoDifference\": true }, \"test_1\": { \"NoDifference\": true }, \"test_2\": { \"NoDifference\": true } } Note pesto test is designed not to fail if the requests pass; Instead it will simply compare dictionaries and display / save the differences as well as the responses, so that the user can go look at what happened and check if this is correct. pesto test should be used for debug purposed and not for unit test purposes. We will see later how we can use the PESTO test API with pytest to actually run unit tests","title":"Using pesto test command"},{"location":"tutorial_pytorch.html#bonus-using-pytest-unit-testing","text":"Once you're sure and have debugged properly you can write or edit unit tests in project-root/tests/ (check the autogenerated file tests/test_service.py ) and run it with pytest tests on your root project This can be used to ensure non regression on further edits or if you want to do test driver development","title":"Bonus: Using Pytest &amp; unit testing"},{"location":"tutorial_pytorch.html#bonus-using-pesto-python-api-to-run-tests-send-requests-to-model","text":"Should you want to use in a non-scalable way or further test your services, you can have a look at the {project-root}/scripts/example_api_usage.py file that exposes the low level python API that is used in pesto test The ServiceManager class is the class used as a proxy for the python Docker API, and is used to pull / run /attach / stop the containers The PayloadGenerator class is used to translate files to actual json payload for the REST API The EndpointManager manages the various endpoints of the processes, and act as a front to post/get requests The ServiceTester is used to validate payloads & responses against their expected values Note This API is a simple example of how to use services packaged with pesto in python scripts. We encourage you to copy/paste and modify the classes should you feel the need for specific use cases, but both this and pesto test is not designed for robustness and scalability We consider the target of pesto test capabilities to be the data scientist, integration testing & scalability should be done at production level","title":"Bonus: Using PESTO Python API to run tests &amp; send requests to model"},{"location":"tutorial_pytorch.html#adding-a-gpu-profile","text":"In order to create an image with GPU support, we can complete the proposed profile gpu . The file requirements.gpu.json can be updated as follows : { \"environments\": {}, \"requirements\": {}, \"dockerBaseImage\": \"pytorch/pytorch:1.5-cuda10.1-cudnn7-runtime\" } You can now build your GPU enabled microservice : pesto build {root of your project}/pytorch-deployment-tutorial -p gpu","title":"Adding a GPU profile"},{"location":"tutorial_pytorch.html#pesto-profiles","text":"in order to accomodate for different hardware targets or slight variations of the same process to deploy, PESTO has a built-in capabilities called profiles Basically, PESTO profiles is a list of ordered strings ( gpu stateless ) whose .json files in build/api folders sequentially update the base file. To use them, simply add the list of profiles to your PESTO commands: pesto build {project-root} -p p1 p2 or pesto test {project-root} -p p1 p2 The profiles json files are named {original_file}.{profile}.json . For example, for a description.json , then the corresponding description.json for the profile gpu would be description.gpu.json . Profile jsons can be partially complete as they only update the base values if the files are present. Example: description.json : {\"key\":\"value\", \"key2\":\"value2\"} description.p1.json : {\"key\":\"value3\"} Then calling pesto build . -p p1 will generate a description.json : {\"key\":\"value3\", \"key2\":\"value2\"} and take all the other files without additionnal profiles. Warning Due to the sequential nature of dictionary updates, the profiles are order dependent If you have a computer with nvidia drivers & a gpu you can try to run pesto build . -p gpu and pesto test . -p gpu --nvidia which should do the same as above but with gpu support (and actually run the process on gpu)","title":"PESTO Profiles"},{"location":"tutorial_pytorch.html#stateful-stateless-services","text":"PESTO supports building stateless services as well as stateful services . With stateless services, it is expected that the processing replies directly to the processing request with the response. These services should have no internal state and should always return the same result when presented with the same payload Stateful services can have internal states, and store the processing results to be later queried. The main difference is that sending a processing request to api/v1/process to a stateful service will not return the result but a jobID . The job state can be quiered at GET api/v1/jobs/{jobID}/status and results can be queried at GET api/v1/jobs/{jobID}/results when the job is done. The response of the latter request will be a json matching the output schema with URI to individual content (that should individually be queried using GET requests ) Try building your previous service with pesto build . -p stateful and starting it as previously, docker run --rm -p 4000:8080 pytorch-deployment-tutorial:1.0.0.dev0-stateful Then, run the API usage script ( python scripts/example_api_usage ) while having modified the image name to stateful. This script should send several requests (like pesto test), but the advantage is that it doesn't kill the service afterwards, so it is possible to look at what happened: Try doing a get request on /api/v1/jobs/ you should see a list of jobs Grab a job id then do a GET request on /api/v1/jobs/{jobID}/status . It should be \"DONE\" Then do a GET request on /api/v1/jobs/{jobID}/results to get results You should get something like { \"category\": \"http://localhost:4000/api/v1/jobs/1080019533/results/category\" } A GET request on the aforementioned URL should return Egyptian_cat or mortar","title":"Stateful &amp; Stateless services"},{"location":"tutorial_pytorch.html#next-steps","text":"You should version your PESTO project using git so that it is reproducible The rest of the documentation should be more accessible now that you have completed this tutorial Feel free to send us feedback and ask any question on github There are some advanced usage & tips in the pesto cookbook . If you find an use case that is not documented, feel free to submit a PR on github to update the documentation","title":"Next Steps"}]}